{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Classification\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** In this chapter, we will tackle the Classification task. Unlike Regression (predicting a value), Classification is about predicting a category (class). We will use the famous **MNIST** dataset, which contains 70,000 images of handwritten digits.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Binary Classifiers:** distinguishing between two classes (e.g., \"Is this a 5?\" vs. \"Not a 5\").\n",
    "* **Performance Measures:** Accuracy is not enough. We need Confusion Matrices, Precision, Recall, and F1 Score.\n",
    "* **The Precision/Recall Trade-off:** Understanding the balance between catching all positive cases and being correct when predicting positive.\n",
    "* **Multiclass Classification:** distinguishing between more than two classes (0, 1, ..., 9).\n",
    "* **Error Analysis:** Analyzing where the model makes mistakes to improve it.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Fetching datasets using `fetch_openml`.\n",
    "* Training `SGDClassifier` and `RandomForestClassifier`.\n",
    "* Using Cross-Validation for accuracy.\n",
    "* Plotting ROC Curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation\n",
    "\n",
    "### 1. MNIST Dataset\n",
    "Often called the \"Hello World\" of Machine Learning. It consists of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "### 2. Binary vs. Multiclass\n",
    "* **Binary Classifier:** Capable of distinguishing between just two classes (e.g., \"5\" and \"Not-5\").\n",
    "* **Multiclass Classifier:** Capable of distinguishing between more than two classes (e.g., digits 0 through 9).\n",
    "Some algorithms (SGD, SVM) are strictly binary classifiers but can be used for multiclass classification using strategies like **OvR** (One-versus-the-Rest) or **OvO** (One-versus-One).\n",
    "\n",
    "### 3. Performance Metrics (Crucial!)\n",
    "**Accuracy** is often a bad metric for classifiers, especially with *skewed datasets* (e.g., if 90% of data is \"Not-5\", a dummy classifier that always guesses \"Not-5\" has 90% accuracy but is useless).\n",
    "\n",
    "* **Confusion Matrix:** A table showing the counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "* **Precision:** Accuracy of positive predictions.\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "* **Recall (Sensitivity):** Ratio of positive instances that are correctly detected.\n",
    "$$ Recall = \\frac{TP}{TP + FN} $$\n",
    "* **F1 Score:** The harmonic mean of Precision and Recall.\n",
    "\n",
    "### 4. ROC Curve\n",
    "The Receiver Operating Characteristic (ROC) curve plots the **True Positive Rate (Recall)** against the **False Positive Rate**. A good classifier stays as far away from the dotted diagonal line (random guessing) as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "We will start by loading the MNIST dataset and building a \"5-detector\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Fetch MNIST dataset\n",
    "# as_frame=False ensures we get numpy arrays (standard for image data in this book)\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(np.uint8) # Convert labels from strings to integers\n",
    "\n",
    "print(\"Data shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Visualize one digit\n",
    "some_digit = X[0]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap=mpl.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Label:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "The MNIST dataset is already split into a training set (first 60,000 images) and a test set (last 10,000 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Binary Classifier (The \"5-Detector\")\n",
    "We simplify the problem to only distinguish between two classes: \"5\" and \"Not-5\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Stochastic Gradient Descent (SGD) classifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n",
    "# Predict the digit we visualized earlier\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "We will calculate the Confusion Matrix, Precision, and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Get predictions using Cross-Validation (cleaner than testing on test set)\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mx = confusion_matrix(y_train_5, y_train_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_mx)\n",
    "\n",
    "# Precision and Recall\n",
    "print(\"Precision:\", precision_score(y_train_5, y_train_pred))\n",
    "print(\"Recall:\", recall_score(y_train_5, y_train_pred))\n",
    "print(\"F1 Score:\", f1_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass task and automatically runs OvR (One-versus-the-Rest) or OvO, depending on the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if the model can classify all digits (0-9)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])\n",
    "\n",
    "# Check the decision function scores for all 10 classes\n",
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "print(\"Scores for each class:\", some_digit_scores)\n",
    "print(\"Predicted class:\", np.argmax(some_digit_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Data Fetching and Reshaping\n",
    "**Input:** `fetch_openml('mnist_784')`.\n",
    "**Process:** We download the dataset. `X` contains the pixel intensities (784 features per image, from 28x28 pixels). `y` contains the labels.\n",
    "**Output:** Arrays `X` (70000, 784) and `y` (70000,).\n",
    "\n",
    "### 2. Binary Training\n",
    "We create a target vector `y_train_5` which is `True` for all 5s and `False` for all other digits. The `SGDClassifier` relies on randomness, so `random_state=42` ensures reproducible results. It finds a linear hyperplane that best separates the 5s from the non-5s.\n",
    "\n",
    "### 3. Confusion Matrix Analysis\n",
    "* **True Negatives (Top-Left):** Non-5s correctly classified as Non-5s.\n",
    "* **False Positives (Top-Right):** Non-5s incorrectly classified as 5s.\n",
    "* **False Negatives (Bottom-Left):** 5s incorrectly classified as Non-5s.\n",
    "* **True Positives (Bottom-Right):** 5s correctly classified as 5s.\n",
    "\n",
    "This matrix tells us *how* the model is failing, not just *that* it is failing.\n",
    "\n",
    "### 4. Multiclass Strategy (OvR)\n",
    "When we run `sgd_clf.fit(X_train, y_train)` with 10 classes, Scikit-Learn actually trains 10 binary classifiers:\n",
    "1. 0-detector\n",
    "2. 1-detector\n",
    "...\n",
    "10. 9-detector\n",
    "\n",
    "When you ask for a prediction, it gets the decision score from all 10 classifiers and picks the class with the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Classification vs Regression:** Classification predicts categories; Regression predicts values.\n",
    "* **MNIST:** The standard dataset for learning image classification basics.\n",
    "* **Accuracy Trap:** Do not rely solely on accuracy for skewed datasets. Use the **Confusion Matrix**.\n",
    "* **Precision vs Recall:**\n",
    "    * Precision: \"When it claims it's a 5, is it really a 5?\"\n",
    "    * Recall: \"Did it find all the 5s?\"\n",
    "    * You can't have both 100%; increasing one usually reduces the other (Trade-off).\n",
    "* **Multiclass:** Algorithms can handle multiple classes natively (Random Forest) or use OvR/OvO strategies (SGD, SVM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
