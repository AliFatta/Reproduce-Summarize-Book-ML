{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Deep Computer Vision Using CNNs\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** In this chapter, we will learn about Convolutional Neural Networks (CNNs). Unlike dense networks that treat every pixel as an independent feature, CNNs understand spatial relationships (e.g., \"this pixel is part of a line, which is part of a square\"). We will start with the building blocks (Convolution, Pooling) and then explore famous architectures like ResNet and Xception. Finally, we will use **Transfer Learning** to use state-of-the-art models for our own tasks.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **The Visual Cortex:** How biological vision inspired CNNs.\n",
    "* **Convolutional Layers:** Filters (Kernels), Feature Maps, Stride, and Padding.\n",
    "* **Pooling Layers:** Max Pooling and Average Pooling for downsampling.\n",
    "* **CNN Architectures:**\n",
    "    * **LeNet-5 (1998):** The grandfather of CNNs.\n",
    "    * **AlexNet (2012):** The deep learning revolution starter.\n",
    "    * **VGGNet:** Simplicity with 3x3 filters.\n",
    "    * **GoogLeNet (Inception):** Using Inception modules to capture features at different scales.\n",
    "    * **ResNet (2015):** Using skip connections to train ultra-deep networks (100+ layers).\n",
    "    * **Xception:** Extreme Inception with depthwise separable convolutions.\n",
    "* **Transfer Learning:** Using a model pretrained on ImageNet (millions of images) to classify specific datasets with high accuracy.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Building a CNN from scratch using `Conv2D` and `MaxPooling2D`.\n",
    "* Implementing a simplified ResNet Unit.\n",
    "* Using Pretrained Keras Models (`Xception`, `ResNet50`) for image classification.\n",
    "* Preprocessing images for specific models (`preprocess_input`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. Convolutional Layers\n",
    "A standard Dense layer connects every input pixel to every neuron. This is wasteful and ignores geometry. A **Convolutional Layer** uses small \"filters\" (or kernels) that slide across the image.\n",
    "\n",
    "**Key Parameters:**\n",
    "* **Filters:** The number of feature maps to output (e.g., 64 filters might learn 64 different edge types).\n",
    "* **Kernel Size:** The size of the sliding window (usually 3x3, 5x5, or 7x7).\n",
    "* **Stride:** How many pixels the filter moves at each step. Stride=1 keeps spatial dimensions roughly the same. Stride=2 halves them.\n",
    "* **Padding:** \n",
    "    * `'valid'`: No padding. The output is smaller than the input.\n",
    "    * `'same'`: Zero padding is added so the output size equals the input size (if stride=1).\n",
    "\n",
    "### 2. Pooling Layers\n",
    "Pooling layers reduce the spatial dimensions (width, height) to reduce computational load and memory usage, and to make the network robust to small shifts in the image (invariance).\n",
    "* **Max Pooling:** Takes the maximum value in the window. Extracts the most prominent feature.\n",
    "* **Average Pooling:** Takes the average. Smoothes the features.\n",
    "* **Global Average Pooling:** Computes the mean of the entire feature map. Often used just before the final classification layer.\n",
    "\n",
    "### 3. ResNet (Residual Networks)\n",
    "As networks got deeper (20+ layers), training became hard due to vanishing gradients. ResNet introduced the **Skip Connection** (or Residual Connection).\n",
    "Instead of trying to learn $h(x)$, the layer tries to learn the residual $f(x) = h(x) - x$. The output becomes $f(x) + x$.\n",
    "If the optimal function is close to the identity function (doing nothing), the weights can easily shrink to zero, letting the signal pass through the skip connection. This allows training networks with 100+ layers.\n",
    "\n",
    "### 4. Transfer Learning\n",
    "Training a CNN from scratch requires huge datasets and massive GPU power. Instead, we use models pre-trained on **ImageNet** (1.2 million images, 1000 classes). \n",
    "We can remove the top layer (classification head) and replace it with our own, then train only that new layer. The lower layers have already learned to detect edges, shapes, and textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Building a CNN from Scratch\n",
    "We will build a CNN to classify Fashion MNIST images. We use `Conv2D` layers followed by `MaxPooling2D` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Preprocessing\n",
    "# CNNs expect 3D inputs (Height, Width, Channels). \n",
    "# Since Fashion MNIST is grayscale, we must add the channel dimension: [28, 28, 1]\n",
    "X_train_full = X_train_full.reshape((60000, 28, 28, 1)) / 255.0\n",
    "X_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Building the Model\n",
    "model = keras.models.Sequential([\n",
    "    # Layer 1: Conv2D with 64 filters, 7x7 kernel. Input shape is mandatory.\n",
    "    keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    \n",
    "    # Layer 2 & 3: Stacked Conv2D with 128 filters, 3x3 kernel\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    \n",
    "    # Layer 4 & 5: Going deeper with 256 filters\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    \n",
    "    # Fully Connected Head\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (Note: CNNs are slower to train than MLPs without a GPU)\n",
    "# We limit epochs to 3 for demonstration purposes\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using Pretrained Models (ResNet-50)\n",
    "We will load a ResNet-50 model trained on ImageNet and use it to classify real-world images. This requires resizing images to 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "# Load ResNet50 model with ImageNet weights\n",
    "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n",
    "\n",
    "# Load sample images (a Flower and a Chinese Temple)\n",
    "images = load_sample_images().images\n",
    "X_raw = np.array(images)\n",
    "\n",
    "# Preprocessing for ResNet50\n",
    "# 1. Resize to 224x224 (ResNet standard)\n",
    "X_resized = tf.image.resize(X_raw, [224, 224])\n",
    "\n",
    "# 2. Use the specific preprocess_input function for ResNet\n",
    "# This function handles scaling (e.g., -1 to 1 or 0 to 1) and channel ordering (RGB vs BGR)\n",
    "inputs = keras.applications.resnet50.preprocess_input(X_resized)\n",
    "\n",
    "# Prediction\n",
    "Y_proba = model.predict(inputs)\n",
    "\n",
    "# Decode predictions into human-readable class names\n",
    "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "\n",
    "for image_index in range(len(images)):\n",
    "    print(f\"Image #{image_index}\")\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f\"  {name} - {class_id}: {y_proba*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transfer Learning with Xception\n",
    "How do we adapt a powerful model like Xception to our own dataset (e.g., flowers)?\n",
    "1.  Load Xception **without the top layer** (`include_top=False`).\n",
    "2.  Freeze the base layers (make them non-trainable).\n",
    "3.  Add our own GlobalAveragePooling and Dense output layer.\n",
    "4.  Train only the new layers.\n",
    "5.  Unfreeze some base layers and fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example setup (we won't run full training here as it requires a large dataset)\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 1. Base Model\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# 2. Add Custom Head\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(avg) # Assuming 10 classes\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# 3. Freeze Base Layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 4. Compile\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 5. Train (Simulated code)\n",
    "# history = model.fit(dataset, epochs=5)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. The CNN Architecture\n",
    "Our Fashion MNIST model follows a classic structure:\n",
    "* **C-P-C-C-P:** Convolution -> Pooling -> Conv -> Conv -> Pooling.\n",
    "* **Filter Pyramid:** We start with 64 filters, then go to 128, then 256. As we go deeper into the network, the spatial dimensions ($28\\times28 \\rightarrow 14\\times14 \\rightarrow 7\\times7$) decrease, but the number of feature maps increases. This means the network learns fewer high-level features (like \"sleeves\" or \"buttons\") compared to many low-level features (like \"lines\" or \"curves\").\n",
    "* **Padding='same':** We keep the size constant during convolution so we don't lose pixels at the borders.\n",
    "\n",
    "### 2. Why ResNet works\n",
    "In standard deep networks, gradients have to multiply through many layers during backpropagation. If layers are weights < 1, the gradient vanishes. In ResNet, the gradient can flow directly through the skip connection ($+ x$) without being attenuated. This acts like a \"highway\" for gradients to reach the early layers.\n",
    "\n",
    "### 3. Preprocessing for Pretrained Models\n",
    "Every model expects input in a specific format. ResNet expects inputs to be zero-centered (mean subtracted). Xception might expect inputs scaled to -1 to 1. \n",
    "Always use the helper function `keras.applications.model_name.preprocess_input(x)` instead of manually scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **CNNs** exploit the spatial structure of images using **Filters** (local patterns) and **Pooling** (subsampling).\n",
    "* **Architectures:** Modern CNNs like **ResNet** and **Xception** are extremely deep but trainable thanks to Residual connections and efficient convolutions.\n",
    "* **Transfer Learning:** The most practical way to use Deep Learning. Download a model trained on ImageNet, remove the top, and train a new classifier on top.\n",
    "* **Data Augmentation:** (Not shown in code but crucial) Shifting, rotating, and flipping images during training to artificially increase dataset size and reduce overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
