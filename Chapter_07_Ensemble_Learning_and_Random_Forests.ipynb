{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** This chapter explores the concept of **Ensemble Learning**: the idea that aggregating the predictions of a group of predictors (such as classifiers or regressors) will often get better results than with the best individual predictor. We will build the powerful **Random Forest** algorithm and explore advanced boosting techniques like **AdaBoost** and **Gradient Boosting**.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Voting Classifiers:** Hard Voting vs. Soft Voting.\n",
    "* **Bagging and Pasting:** Training the same algorithm on different random subsets of data.\n",
    "* **Out-of-Bag (OOB) Evaluation:** A clever way to validate Bagging models without a separate validation set.\n",
    "* **Random Forests:** Combining Bagging with feature randomization.\n",
    "* **Extra-Trees:** Extremely Randomized Trees for faster training.\n",
    "* **Boosting:** Training predictors sequentially to correct the mistakes of previous ones (AdaBoost, Gradient Boosting).\n",
    "* **Stacking:** Using a \"meta-learner\" to learn how to combine predictions.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Implementing `VotingClassifier` in Scikit-Learn.\n",
    "* Using `BaggingClassifier` to reduce variance.\n",
    "* Visualizing Feature Importance with Random Forests.\n",
    "* Implementing Early Stopping with Gradient Boosting to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. The Wisdom of the Crowd (Voting Classifiers)\n",
    "Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases, you will find that this aggregated answer is better than an expert's answer. This is the foundation of Ensemble Learning.\n",
    "\n",
    "**Why does this work? (Law of Large Numbers)**\n",
    "Imagine a slightly biased coin that has a 51% chance of coming up heads. If you toss it 1,000 times, the ratio of heads will likely be close to 51%. If you toss it 10,000 times, the probability of getting a majority of heads climbs to over 97%. \n",
    "Similarly, if you have 1,000 classifiers that are each individually weak (only slightly better than random guessing, say 51% accuracy), combining them into an ensemble can produce a strong classifier with high accuracy, provided that:\n",
    "1.  The models are sufficiently independent.\n",
    "2.  They make uncorrelated errors (they don't all fail on the same difficult instances).\n",
    "\n",
    "**Types of Voting:**\n",
    "* **Hard Voting:** Each classifier votes for a class (e.g., \"Class A\"). The ensemble picks the class with the most votes (Majority Rule).\n",
    "* **Soft Voting:** If all classifiers can estimate class probabilities (i.e., they have a `predict_proba()` method), the ensemble averages the probabilities for each class and picks the class with the highest average probability. This usually performs better than hard voting because it gives more weight to highly confident votes.\n",
    "\n",
    "### 2. Bagging and Pasting\n",
    "Another approach is to use the *same* training algorithm for every predictor but train them on different random subsets of the training set.\n",
    "\n",
    "* **Bagging (Bootstrap Aggregating):** Sampling is performed *with replacement*. This means the same training instance can be sampled several times for the same predictor. This introduces more diversity in the subsets, which generally results in slightly higher bias but significantly lower variance (less overfitting).\n",
    "* **Pasting:** Sampling is performed *without replacement*.\n",
    "\n",
    "**Out-of-Bag (OOB) Evaluation:**\n",
    "In Bagging, some instances may be sampled several times for a given predictor, while others may not be sampled at all. The instances that are NOT sampled (about 37% on average) are called \"Out-of-Bag\" instances. Since the predictor never saw these during training, they can be used as a validation set. We can evaluate the ensemble by averaging the OOB evaluations of each predictor, removing the need for a separate validation set.\n",
    "\n",
    "### 3. Random Forests\n",
    "A Random Forest is essentially an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with `max_samples` set to the size of the training set.\n",
    "\n",
    "**Extra Randomness:**\n",
    "Random Forests add an extra layer of randomness. Instead of searching for the very best feature when splitting a node (like a standard Decision Tree), it searches for the best feature among a *random subset of features*. \n",
    "* This results in greater tree diversity.\n",
    "* It trades higher bias for lower variance, generally yielding a better overall model.\n",
    "\n",
    "**Feature Importance:**\n",
    "A great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). \n",
    "\n",
    "### 4. Boosting\n",
    "Boosting (Hypothesis Boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea is to train predictors sequentially, each trying to correct the predecessor.\n",
    "\n",
    "**a. AdaBoost (Adaptive Boosting):**\n",
    "To correct the predecessor, AdaBoost pays more attention to the training instances that the predecessor underfitted. \n",
    "1.  Train a base classifier (e.g., Decision Stump).\n",
    "2.  Identify the misclassified instances.\n",
    "3.  Increase the relative weight of those misclassified instances.\n",
    "4.  Train a second classifier using the updated weights.\n",
    "5.  Repeat.\n",
    "The final prediction is a weighted vote, where valid classifiers (accurate ones) have more weight.\n",
    "\n",
    "**b. Gradient Boosting:**\n",
    "Like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking instance weights, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.\n",
    "* Step 1: Train `Tree_1` on $(X, y)$.\n",
    "* Step 2: Calculate errors $y_{residual1} = y - Tree_1(X)$.\n",
    "* Step 3: Train `Tree_2` on $(X, y_{residual1})$.\n",
    "* Step 4: Calculate errors $y_{residual2} = y_{residual1} - Tree_2(X)$.\n",
    "* Final Prediction: $y_{pred} = Tree_1(X) + Tree_2(X) + \\dots$\n",
    "\n",
    "**XGBoost:** A scalable and highly optimized implementation of Gradient Boosting that is very popular in competitions.\n",
    "\n",
    "### 5. Stacking (Stacked Generalization)\n",
    "Instead of using trivial functions (like hard voting) to aggregate the predictions of all predictors in an ensemble, why not train a model to perform this aggregation?\n",
    "* **Base Learners:** The initial models (e.g., SVM, Tree, KNN).\n",
    "* **Meta Learner (Blender):** The final model that takes the predictions of the base learners as inputs and outputs the final prediction.\n",
    "* **Hold-out Set:** Stacking typically requires splitting the training set into two. The first part is used to train the base learners. The second part is used to generate predictions from the base learners, which then become the \"training data\" for the meta-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Voting Classifiers\n",
    "We will compare individual classifiers (Logistic Regression, Random Forest, SVC) against a Voting Classifier on the Moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42) # probability=True needed for soft voting\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft' # Try 'hard' for majority rule, 'soft' for probability averaging\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bagging and Out-of-Bag Evaluation\n",
    "We use `BaggingClassifier` with 500 Decision Trees. We enable `oob_score=True` to evaluate performance without a separate validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True, random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"OOB Score (Estimated Validation Accuracy):\", bag_clf.oob_score_)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forests and Feature Importance\n",
    "We train a Random Forest on the Iris dataset to see which features matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 AdaBoost\n",
    "We use a Decision Tree with `max_depth=1` (a Decision Stump) as the weak learner. AdaBoost will sequentially add stumps that focus on the errors of the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, ada_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Gradient Boosting with Early Stopping\n",
    "Here we manually implement Early Stopping. We train a Gradient Boosting Regressor with 120 trees, but we measure the validation error at each stage (after 1 tree, after 2 trees, etc.) and pick the best number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate noisy quadratic data\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# Find the optimal number of trees\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best number of trees: {bst_n_estimators}\")\n",
    "print(f\"Minimum MSE: {np.min(errors)}\")\n",
    "\n",
    "# Plotting the error curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators-1, bst_n_estimators-1], [0, np.min(errors)], \"k--\")\n",
    "plt.plot(bst_n_estimators-1, np.min(errors), \"ko\")\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.title(\"Validation Error vs Number of Trees\")\n",
    "\n",
    "plt.subplot(122)\n",
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    plt.axis(axes)\n",
    "\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble prediction\")\n",
    "plt.title(f\"Best Model ({bst_n_estimators} trees)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Voting Classifier Analysis\n",
    "**Input:** We have three diverse classifiers: Logistic Regression (linear), Random Forest (ensemble of trees), and SVM (nonlinear kernel).\n",
    "**Process:**\n",
    "* `VotingClassifier` trains all three models on the training data.\n",
    "* When predicting, if `voting='soft'`, it asks each model for the probability of class 0 and class 1.\n",
    "* Example: LR says 60% Class 1, RF says 80% Class 1, SVM says 40% Class 1. Average = (60+80+40)/3 = 60%. The ensemble predicts Class 1.\n",
    "**Output:** The Voting Classifier achieves slightly higher accuracy (e.g., 91.2%) compared to the individual models (e.g., 86%, 89%, 89%). This confirms the \"Wisdom of the Crowd\".\n",
    "\n",
    "### 2. Bagging with OOB\n",
    "**Input:** 500 Decision Trees. `max_samples=100` means each tree is trained on a small random subset of 100 instances.\n",
    "**Process:** \n",
    "* The `BaggingClassifier` builds 500 trees in parallel (`n_jobs=-1`).\n",
    "* Because `bootstrap=True`, about 37% of training data was never seen by Tree #1, another set of 37% was never seen by Tree #2, etc.\n",
    "* `oob_score_` calculates the accuracy by testing Tree #1 only on the data it didn't see, and averaging this process across all trees.\n",
    "**Output:** The OOB score (e.g., 0.904) is very close to the actual Test Set accuracy (e.g., 0.912). This proves OOB is a reliable validation metric.\n",
    "\n",
    "### 3. Feature Importance\n",
    "**Process:** The Random Forest checks every split in every tree. If splitting on \"Petal Length\" reduces the Gini impurity significantly, its score goes up.\n",
    "**Output:** We clearly see that Petal Length and Petal Width are the most important features (score > 0.4), while Sepal Length and Width are much less relevant. This serves as automatic **Feature Selection**.\n",
    "\n",
    "### 4. Gradient Boosting & Early Stopping\n",
    "**Concept:** Overfitting happens when we add too many trees. The model starts memorizing the noise in the training data.\n",
    "**Process:**\n",
    "* We train 120 trees.\n",
    "* `staged_predict` allows us to measure the validation error after 1 tree, 2 trees, ..., 120 trees.\n",
    "* We plot this error. We see it goes down initially (learning), reaches a minimum, and then starts going up (overfitting).\n",
    "**Output:** We select the number of trees corresponding to the minimum error (e.g., 55 trees) to get the optimal model that generalizes best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Ensemble Methods** combine multiple weak learners to form a strong learner.\n",
    "* **Bagging:** Reduces variance (overfitting) by training parallel models on random subsets. Example: Random Forests.\n",
    "* **Random Forests:** Powerful, versatile, and provide Feature Importance out-of-the-box. Uses two levels of randomness (data sampling + feature sampling).\n",
    "* **Boosting:** Reduces bias (underfitting) by training sequential models that correct previous mistakes. Example: AdaBoost, Gradient Boosting.\n",
    "* **Stacking:** Uses a meta-model to learn how to best combine the predictions of base models, often outperforming simple voting.\n",
    "* **Trade-off:** Ensembles are usually more accurate but slower to train and deploy than individual models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
