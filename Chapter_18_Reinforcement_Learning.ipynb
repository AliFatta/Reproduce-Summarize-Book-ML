{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Reinforcement Learning\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Reinforcement Learning (RL) is the technology behind AlphaGo, self-driving cars, and robots that can walk. In this chapter, we will build agents that can learn to balance a pole on a cart (CartPole) and play Atari games by using Deep Q-Learning (DQN).\n",
    "\n",
    "**Key Concepts:**\n",
    "* **RL Components:** Agent, Environment, Action, State, Reward.\n",
    "* **Policy Search:** Finding the best strategy (policy) to maximize rewards.\n",
    "* **Credit Assignment Problem:** Determining which action caused the reward (was it the move 10 steps ago?).\n",
    "* **Markov Decision Processes (MDPs):** The mathematical framework for RL.\n",
    "* **Q-Learning:** Learning the \"quality\" (value) of every action in every state.\n",
    "* **Deep Q-Networks (DQN):** Using a neural network to approximate Q-values for complex environments.\n",
    "* **Exploration vs. Exploitation:** Balancing trying new things (random actions) vs. using known best actions.\n",
    "* **Replay Buffer:** Storing past experiences to train the network stably.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Using **OpenAI Gym** to render and interact with environments.\n",
    "* Implementing a hard-coded policy for CartPole.\n",
    "* Building a **DQN** Agent with Keras.\n",
    "* Implementing a **Custom Training Loop** for RL (collect data -> train -> repeat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To run this chapter, you need OpenAI Gym\n",
    "# pip install gym\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. The RL Cycle\n",
    "1.  **Observation:** The Agent sees the current state $S_t$ of the environment (e.g., pixel image of a game).\n",
    "2.  **Action:** The Agent selects an action $A_t$ based on its Policy $\\pi$.\n",
    "3.  **Step:** The Environment processes the action, transitions to a new state $S_{t+1}$, and returns a Reward $R_{t+1}$ (e.g., +1 for survival, -1 for crashing).\n",
    "4.  **Repeat:** The goal is to maximize the expected sum of future rewards (Return).\n",
    "\n",
    "### 2. Markov Decision Processes (MDP)\n",
    "RL assumes the environment satisfies the **Markov Property**: The future depends only on the current state, not the history. \n",
    "We use the **Bellman Optimality Equation** to find the optimal Q-Value $Q^*(s, a)$:\n",
    "$$ Q^*(s, a) = R(s, a) + \\gamma \\max_{a'} Q^*(s', a') $$\n",
    "Where $\\gamma$ (gamma) is the discount factor (0 to 1). If $\\gamma=0$, the agent is short-sighted. If $\\gamma=1$, it cares infinitely about the future.\n",
    "\n",
    "### 3. Deep Q-Learning (DQN)\n",
    "In simple games (Tic-Tac-Toe), we can store a table of Q-values for every state. In complex games (Pacman, Go), states are infinite. \n",
    "We use a **Deep Neural Network** (DQN) to *approximate* the Q-value function: $Q(s, a) \\approx DNN(s)$.\n",
    "Input: State (Image/Vector). Output: Q-values for each possible action.\n",
    "\n",
    "### 4. Experience Replay\n",
    "Neural networks hate correlated data (sequential frames of a game are highly correlated). To fix this, we store the agent's experiences $(S, A, R, S')$ in a massive buffer (replay memory) and train the network on small **random batches** sampled from this buffer. This breaks correlation and stabilizes training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 The CartPole Environment\n",
    "The goal is to balance a pole on a moving cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "print(\"Observation (Cart Pos, Cart Vel, Pole Angle, Pole Vel):\", obs)\n",
    "\n",
    "# Render the environment (Note: Rendering might not work in headless cloud notebooks like Colab without extra setup)\n",
    "# env.render()\n",
    "\n",
    "# Hard-coded policy: If pole tilts left, push cart left.\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1 # 0 = Left, 1 = Right\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "print(\"Mean Reward (Hard-coded):\", np.mean(totals), \"(Max is 200.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building a DQN Agent\n",
    "We will build a neural network that takes the observation (4 floats) and outputs 2 Q-values (Left, Right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [4] # 4 observations\n",
    "n_outputs = 2 # 2 actions\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2) # Exploration\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0]) # Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Replay Buffer\n",
    "We use a `deque` to store the last 2000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Custom Training Loop (The DQN Algorithm)\n",
    "We train the model to predict the Target Q-Value, which is calculated using the Bellman Equation: `Reward + gamma * max(Next_Q_Value)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Compute target Q values\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards + \n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        # We only care about the Q-value of the action we actually took\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Running the Training\n",
    "We run 600 episodes. We start with high exploration (epsilon=1) and decay it to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "best_score = 0\n",
    "\n",
    "for episode in range(600):\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_score = step\n",
    "    \n",
    "    # Train only after we have enough data\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode: {episode}, Best Score: {best_score}, Mean Reward: {np.mean(rewards[-50:])}\")\n",
    "\n",
    "print(\"Training Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Epsilon-Greedy Policy\n",
    "At the beginning of training, the model knows nothing (random weights). If we just follow the model's predictions (Exploitation), the agent will repeat the same stupid mistakes forever.\n",
    "We need **Exploration**: forcing the agent to take random actions to discover new states. \n",
    "* `epsilon=1.0`: 100% random actions.\n",
    "* `epsilon=0.01`: 1% random, 99% best action.\n",
    "We decay epsilon over time as the model gets smarter.\n",
    "\n",
    "### 2. The Training Step (Bellman Logic)\n",
    "1.  **Predict:** The model estimates Q-values for the *next state* ($S_{t+1}$).\n",
    "2.  **Target:** We assume the best future action will be taken. So the \"Ground Truth\" value of the current action is: *Immediate Reward + Discounted Future Reward*.\n",
    "3.  **Mask:** The model outputs Q-values for ALL actions (Left, Right). But we only executed ONE action. We use a mask (one-hot vector) to zero out the Q-value of the action we didn't take, so it doesn't affect the loss calculation.\n",
    "4.  **Backprop:** We minimize the difference (MSE) between the Model's guess for the current Q-value and the Target Q-value calculated from the actual reward received.\n",
    "\n",
    "### 3. Stability Issues\n",
    "RL is notoriously unstable. If the model updates its weights, the \"Target\" values (which are calculated using the model itself!) shift immediately. It's like a dog chasing its own tail. \n",
    "* **Solution (Target Network):** In production DQN (like DeepMind's), we use two networks: one for choosing actions (Online) and a frozen copy for calculating targets (Target). We copy the weights from Online to Target every 1000 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **RL** is about learning from consequences (Rewards).\n",
    "* **Policy:** The strategy the agent follows.\n",
    "* **Q-Learning:** Finds the optimal policy by learning the value of state-action pairs.\n",
    "* **DQN:** Adapts Q-Learning to complex environments by using a Neural Network to approximate the Q-table.\n",
    "* **Replay Buffer:** Essential for breaking data correlations and stabilizing training.\n",
    "* **Epsilon-Greedy:** Essential for balancing exploration and exploitation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
