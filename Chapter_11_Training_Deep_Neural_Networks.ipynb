{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Training a shallow neural network is easy. But training a deep network (with 10+ layers) introduces new challenges: gradients disappearing (vanishing) or growing too large (exploding), extremely slow training, and massive overfitting. This chapter introduces the techniques used to build state-of-the-art Deep Learning models.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Vanishing/Exploding Gradients:** Why deep networks fail to learn and how to fix it using proper Initialization (Glorot, He) and Activation Functions (ELU, SELU, ReLU).\n",
    "* **Batch Normalization:** A technique to stabilize training by normalizing inputs at each layer.\n",
    "* **Transfer Learning:** Reusing parts of a pretrained neural network for a new task.\n",
    "* **Faster Optimizers:** Moving beyond SGD (Momentum, RMSProp, Adam, Nadam).\n",
    "* **Learning Rate Scheduling:** Adjusting the learning speed during training.\n",
    "* **Regularization:** Techniques like Dropout, Alpha Dropout, and Max-Norm to generalize better.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Implementing **He Initialization** and **ELU** activation.\n",
    "* Adding **BatchNormalization** layers to a Keras model.\n",
    "* Using **Adam** optimizer with a Learning Rate Scheduler.\n",
    "* Implementing **Dropout** to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation\n",
    "\n",
    "### 1. Vanishing and Exploding Gradients\n",
    "During backpropagation, gradients propagate from the output layer to the input layer. In deep networks, these gradients often get smaller and smaller until they vanish (the lower layers stop learning), or they grow huge and the weights explode.\n",
    "* **Cause:** Using the Sigmoid activation function (which saturates at 0 and 1) and random weight initialization often resulted in the variance of outputs dying out.\n",
    "* **Solution 1 (Initialization):** Use **Xavier/Glorot Initialization** for Sigmoid/Tanh, and **He Initialization** for ReLU/variants. This ensures variance remains constant across layers.\n",
    "* **Solution 2 (Activation):** Avoid Sigmoid. Use **ReLU** (fast, but can die), **Leaky ReLU** (never dies), **ELU** (smoother than ReLU), or **SELU** (self-normalizing).\n",
    "\n",
    "### 2. Batch Normalization (BN)\n",
    "Even with good initialization, the distribution of inputs to a layer changes during training. BN fixes this by adding an operation before (or after) the activation function to center and normalize the inputs. It learns scaling and shifting parameters.\n",
    "* **Benefit:** Drastically speeds up training and makes the model less sensitive to initialization.\n",
    "\n",
    "### 3. Faster Optimizers\n",
    "Standard SGD is slow. Advanced optimizers speed up convergence:\n",
    "* **Momentum Optimization:** Like a ball rolling down a hill, it gains speed.\n",
    "* **RMSProp:** Adapts the learning rate for each parameter.\n",
    "* **Adam:** Combines Momentum and RMSProp. It is the default choice for many DL tasks.\n",
    "\n",
    "### 4. Regularization (Dropout)\n",
    "Deep nets have millions of parameters and easily overfit. **Dropout** is a simple yet powerful technique: at every training step, every neuron has a probability $p$ (e.g., 50%) of being temporarily \"dropped out\" (ignored). This forces the network to learn robust features that don't rely on specific neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Dealing with Vanishing Gradients\n",
    "We will load Fashion MNIST again and create a deep network (20 layers) to demonstrate modern configuration: **He Initialization** + **ELU Activation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# Adding 20 dense layers with He Normal init and ELU activation\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Note: Without He Init and ELU, a 20-layer network would struggle to train.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Batch Normalization\n",
    "To make the model even more stable, we add `BatchNormalization` layers. The authors of the BN paper argue it should be added *before* the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(), # Batch Norm on input\n",
    "    keras.layers.Dense(300, use_bias=False), # Bias is handled by BN, so use_bias=False\n",
    "    keras.layers.BatchNormalization(), # Batch Norm before activation\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history_bn = model.fit(X_train, y_train, epochs=5,\n",
    "                       validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Faster Optimizers & Learning Rate Scheduling\n",
    "We will use the **Adam** optimizer, which is generally faster than SGD. We will also implement a **Learning Rate Scheduler** that reduces the learning rate exponentially as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a scheduling function\n",
    "def exponential_decay(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay)\n",
    "\n",
    "# Use Adam Optimizer\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history_adam = model.fit(X_train, y_train, epochs=5,\n",
    "                         validation_data=(X_valid, y_valid),\n",
    "                         callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Regularization: Dropout\n",
    "To prevent overfitting, we add Dropout layers. A rate of 0.2 means 20% of neurons are dropped during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history_dropout = model.fit(X_train, y_train, epochs=5,\n",
    "                            validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Initialization and Activation\n",
    "**Problem:** In a 20-layer network with standard initialization, the signal dies out before reaching the bottom layers.\n",
    "**Solution:**\n",
    "* `kernel_initializer=\"he_normal\"`: Initializes weights with a variance that considers the number of inputs, ensuring the signal variance remains constant.\n",
    "* `activation=\"elu\"`: Unlike ReLU, ELU has negative values for $z<0$, which allows the mean unit activation to be closer to 0, helping gradients flow.\n",
    "\n",
    "### 2. Batch Normalization Structure\n",
    "The sequence `Dense` -> `BatchNormalization` -> `Activation` is the textbook way to apply BN. \n",
    "* `use_bias=False`: The `Dense` layer normally adds a bias ($wX + b$). But `BatchNormalization` also includes a shifting parameter (beta). Two bias terms are redundant, so we tell the Dense layer not to use one.\n",
    "\n",
    "### 3. Adam and Scheduling\n",
    "* **Adam:** You rarely need to tune the momentum parameters of Adam; default values work great.\n",
    "* **Scheduler:** We defined a function `exponential_decay` that lowers the learning rate every epoch. This allows the model to make big steps at the start (to learn fast) and tiny steps at the end (to converge precisely to the minimum).\n",
    "\n",
    "### 4. Dropout Behavior\n",
    "During **training**, the `Dropout` layer randomly sets 20% of inputs to zero. \n",
    "During **testing/prediction**, the `Dropout` layer does nothing; it lets all signals pass through (but scales them down by the keeping probability to preserve expected values).\n",
    "This forces the network to become like an ensemble of smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Initialization:** Use **He Initialization** for ReLU-based networks.\n",
    "* **Activation:** Prefer **ELU** or **SELU** over ReLU for deep networks. Avoid Sigmoid.\n",
    "* **Normalization:** Use **Batch Normalization** to train deep networks significantly faster and more stably.\n",
    "* **Optimization:** Use **Adam** or **Nadam** instead of standard SGD.\n",
    "* **Regularization:** Use **Dropout** to fight overfitting in large networks.\n",
    "* **Transfer Learning:** Reuse lower layers of trained networks when you have limited data for a similar task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
