{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Training with TensorFlow\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Up until now, we have used the high-level Keras API (`Sequential`, `Functional`). But what if you need a loss function that doesn't exist in Keras? Or a layer that behaves differently? Or a training loop that does something exotic? This chapter teaches you how to use TensorFlow's lower-level API to customize every part of your Deep Learning pipeline.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **TensorFlow Basics:** Tensors, Operations, and how they differ from NumPy.\n",
    "* **Custom Loss Functions:** Defining your own mathematical criteria for errors.\n",
    "* **Custom Layers:** Building layers with internal weights (kernels/biases) using the Subclassing API.\n",
    "* **Custom Models:** Creating complex architectures that behave dynamically.\n",
    "* **Automatic Differentiation:** Using `GradientTape` to manually compute gradients.\n",
    "* **Custom Training Loops:** Writing the `for` loop for training from scratch, bypassing `model.fit()`.\n",
    "* **TensorFlow Functions:** Using `@tf.function` to compile Python code into efficient TensorFlow Graphs (AutoGraph).\n",
    "\n",
    "**Practical Skills:**\n",
    "* Manipulating Tensors (slicing, reshaping, math operations).\n",
    "* Implementing a **Huber Loss** function manually.\n",
    "* Creating a custom **Dense Layer** from scratch.\n",
    "* Writing a full training loop using `tape.gradient` and `optimizer.apply_gradients`.\n",
    "* debugging TensorFlow code and optimizing it with Graph Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. Tensors and Operations\n",
    "At the heart of TensorFlow is the **Tensor**. It is very similar to a NumPy `ndarray`, but with two critical differences:\n",
    "1.  **Immutability:** You cannot modify a Tensor in place (unlike NumPy arrays). You must create a new one.\n",
    "2.  **GPU Acceleration:** Tensors can immediately be processed by a GPU or TPU, whereas NumPy arrays live on the CPU.\n",
    "\n",
    "**Variables:** Since Tensors are immutable, how do we store weights that need to change during training? We use `tf.Variable`. These are mutable containers for Tensors.\n",
    "\n",
    "### 2. Custom Components (Losses, Layers, Models)\n",
    "* **Loss Function:** A simple Python function that takes `y_true` and `y_pred` and returns a scalar tensor (the error). If you need internal parameters (hyperparameters) inside your loss, you subclass `keras.losses.Loss`.\n",
    "* **Layers:** To build a custom layer with weights, you subclass `keras.layers.Layer` and implement three methods:\n",
    "    * `__init__`: Save hyperparameters.\n",
    "    * `build(input_shape)`: Create the weights (kernels, biases) knowing the input shape. This happens only once.\n",
    "    * `call(inputs)`: The forward pass computation.\n",
    "* **Models:** Subclass `keras.models.Model`. Similar to layers, but usually contains other layers.\n",
    "\n",
    "### 3. Automatic Differentiation (Autodiff)\n",
    "How does TensorFlow know how to compute gradients for *any* custom function you write? It uses **Reverse-Mode Autodiff**.\n",
    "In TF 2.0, this is handled by `tf.GradientTape`. Any operation performed inside a `with tf.GradientTape() as tape:` block is recorded. Afterward, you call `tape.gradient(target, sources)` to get the derivatives.\n",
    "\n",
    "### 4. TensorFlow Functions (Graph Mode)\n",
    "Python is flexible but slow. TensorFlow Graphs are fast and portable. \n",
    "In TF 1.x, you had to manually build graphs. In TF 2.x, Eager Execution (running like standard Python) is default. However, for performance, we often want to convert our functions into Graphs. \n",
    "We use the decorator `@tf.function`. It analyzes your Python code (including `for` loops and `if` statements) and generates an optimized computation graph (**AutoGraph**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Tensors and NumPy\n",
    "Let's explore the basics of Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tensor\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(\"Tensor:\\n\", t)\n",
    "print(\"Shape:\", t.shape)\n",
    "print(\"Dtype:\", t.dtype)\n",
    "\n",
    "# Indexing (same as NumPy)\n",
    "print(\"Slice:\", t[:, 1:])\n",
    "\n",
    "# Operations\n",
    "print(\"Addition:\", t + 10)\n",
    "print(\"Square:\", tf.square(t))\n",
    "print(\"Matrix Multiplication:\", t @ tf.transpose(t)) # @ operator is matmul\n",
    "\n",
    "# TensorFlow and NumPy interoperability\n",
    "a = t.numpy() # Convert to numpy\n",
    "print(\"NumPy array:\", a)\n",
    "t_from_np = tf.constant(np.array([1, 2, 3])) # Convert from numpy\n",
    "print(\"Tensor from NumPy:\", t_from_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Custom Loss Function\n",
    "We will implement the **Huber Loss**, which is less sensitive to outliers than MSE. \n",
    "$$ L_\\delta(y, f(x)) = \\begin{cases} \\frac{1}{2}(y-f(x))^2 & \\text{for } |y-f(x)| \\le \\delta, \\\\ \\delta |y-f(x)| - \\frac{1}{2}\\delta^2 & \\text{otherwise.} \\end{cases} $$\n",
    "We will train a simple model on the California Housing dataset using this custom loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare Data\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define Huber Loss Function\n",
    "def my_huber_loss(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    # tf.where selects elements from squared_loss or linear_loss based on condition\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# Compile and Train with custom loss\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=my_huber_loss, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Custom Layers\n",
    "Let's build a simplified `Dense` layer from scratch to understand the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        # Create weights (kernel) and bias\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # Must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        # Forward pass: X * W + b\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "# Testing the custom layer\n",
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2, verbose=0)\n",
    "print(\"Custom Layer Model trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Automatic Differentiation (GradientTape)\n",
    "Let's calculate the derivative of $f(w) = 3w^2 + 2w + 5$ at $w=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "\n",
    "print(\"Function z = 3*w1^2 + 2*w1*w2\")\n",
    "print(\"dz/dw1 at (5,3):\", gradients[0].numpy()) # Should be 6*5 + 2*3 = 36\n",
    "print(\"dz/dw2 at (5,3):\", gradients[1].numpy()) # Should be 2*5 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Custom Training Loop\n",
    "This is the ultimate level of control. We will not use `model.fit()`. Instead, we will write the loop that iterates over the dataset, computes gradients, and updates weights manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Model and Optimizer\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])\n",
    "\n",
    "# Helper function to sample random batches\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "mean_loss = keras.metrics.Mean()\n",
    "\n",
    "print(\"Starting Custom Training Loop...\")\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        # A. Sampling\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        \n",
    "        # B. Forward Pass & Recording Operations\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True) # training=True is important for Dropout/BN\n",
    "            main_loss = loss_fn(y_batch, y_pred)\n",
    "            loss = main_loss + tf.add_n(model.losses) # Add regularization losses\n",
    "        \n",
    "        # C. Backward Pass (Compute Gradients)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # D. Optimizer Step (Update Weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # E. Update Metrics (for display)\n",
    "        mean_loss(loss)\n",
    "        print_status_bar(step, n_steps, mean_loss)\n",
    "    \n",
    "    # Reset metrics at end of epoch\n",
    "    mean_loss.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Vectorized Huber Loss\n",
    "**Logic:** We want a loss that behaves quadratically for small errors (like MSE) but linearly for large errors (like MAE). \n",
    "**Implementation:** We calculate `error = y_true - y_pred`. `tf.where(condition, x, y)` works like a vectorized if-else statement. It checks every element in the error tensor; if the error is small (< 1), it applies the quadratic formula; otherwise, it applies the linear formula. This creates a robust loss function entirely using TensorFlow operations, meaning it's differentiable and GPU-compatible.\n",
    "\n",
    "### 2. Custom Layer Anatomy\n",
    "* `build()`: This is where we create variables. We use `add_weight`. It's crucial to do this in `build` and not `__init__` because in `__init__`, we don't know the input shape yet (how many neurons represent the input). Keras calls `build` the first time data passes through the layer.\n",
    "* `call()`: This effectively performs the matrix multiplication $X \\cdot W + b$. We then pass it through the activation function.\n",
    "\n",
    "### 3. The Custom Loop\n",
    "This loop replaces the `fit()` method entirely.\n",
    "1.  **GradientTape:** We wrap the forward pass (`model(X_batch)`) and the loss calculation inside the `with` block. TF records every operation involving a Variable.\n",
    "2.  **Regularization:** `model.losses` contains the regularization penalties (L2 loss) we added when defining the layers. We must manually add this to our main loss, or regularization won't happen.\n",
    "3.  **Tape.gradient:** TF traverses the graph backwards from `loss` to `model.trainable_variables` using the chain rule to find the direction to move the weights.\n",
    "4.  **Optimizer.apply_gradients:** This subtracts the gradients (multiplied by learning rate) from the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **TensorFlow Low-Level API:** Gives you full control over tensors, operations, and graphs.\n",
    "* **Customizing:** You can subclass `Loss`, `Layer`, `Model`, and `Metric` to create components that don't exist in the standard library.\n",
    "* **GradientTape:** The engine of Deep Learning in TF2. It enables automatic differentiation for custom training flows.\n",
    "* **Custom Training Loops:** Necessary for advanced research (e.g., GANs, Reinforcement Learning) where the standard `fit()` method is too rigid.\n",
    "* **Performance:** Use `@tf.function` to compile your custom python functions into high-performance TensorFlow Graphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
