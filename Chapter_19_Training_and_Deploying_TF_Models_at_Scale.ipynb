{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Building a great model is only half the battle. The other half is deploying it to production so users can actually use it. In this final chapter, we explore how to export models, serve them via a REST API using **TensorFlow Serving**, deploy them to mobile devices using **TensorFlow Lite**, and speed up training using **Distributed Strategies** (multi-GPU/multi-node).\n",
    "\n",
    "**Key Concepts:**\n",
    "* **SavedModel Format:** The standard, language-neutral format for shipping TF models.\n",
    "* **TensorFlow Serving:** A high-performance serving system for production environments (Docker-based).\n",
    "* **Google Cloud AI Platform:** Deploying models to the cloud (serverless).\n",
    "* **TensorFlow Lite:** Optimizing models for mobile and edge devices (Quantization).\n",
    "* **Distribution Strategies:**\n",
    "    * **MirroredStrategy:** Synchronous training on one machine with multiple GPUs.\n",
    "    * **MultiWorkerMirroredStrategy:** Synchronous training across multiple machines.\n",
    "    * **TPUStrategy:** Training on Google Cloud TPUs.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Exporting a Keras model to `SavedModel` format.\n",
    "* Installing and running TensorFlow Serving with Docker.\n",
    "* Querying the model via REST API.\n",
    "* Converting a model to TFLite and optimizing it.\n",
    "* Writing code that trains on multiple GPUs automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. The SavedModel Format\n",
    "When you use `model.save(\"my_model\")`, TF creates a directory containing:\n",
    "* `saved_model.pb`: The protocol buffer file describing the graph structure (computation logic).\n",
    "* `variables/`: A directory containing the weights.\n",
    "* `assets/`: External files (like vocabulary text files).\n",
    "This format is **universal**. You can train in Python and load the model in Java, C++, or Go.\n",
    "\n",
    "### 2. TensorFlow Serving\n",
    "Serving a model isn't just about writing a Flask app wrapper. In production, you need:\n",
    "* **Versioning:** Serving multiple versions (v1, v2) simultaneously (e.g., for A/B testing).\n",
    "* **Batching:** Grouping requests from different users into a single batch to maximize GPU utilization.\n",
    "* **Performance:** C++ core for ultra-low latency.\n",
    "TF Serving handles all this. It is usually deployed via Docker.\n",
    "\n",
    "### 3. TensorFlow Lite (TFLite)\n",
    "Mobile phones have limited battery and compute power. TFLite is a lightweight library for on-device inference.\n",
    "* **Converter:** Converts `SavedModel` to `.tflite` (FlatBuffer format).\n",
    "* **Quantization:** Reduces weights from 32-bit floats to 8-bit integers (int8). This shrinks the model size by 4x and speeds up inference, with often negligible accuracy loss.\n",
    "\n",
    "### 4. Distributed Training\n",
    "**Data Parallelism (MirroredStrategy):**\n",
    "* You have 4 GPUs.\n",
    "* The model is replicated (mirrored) on every GPU.\n",
    "* A global batch of data (e.g., 128 images) is split into 4 mini-batches of 32.\n",
    "* Each GPU computes gradients on its mini-batch.\n",
    "* Gradients are aggregated (summed) across all GPUs, and weights are updated synchronously.\n",
    "* Effectively, you are training with a batch size of 128, but 4x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Training and Exporting a Model\n",
    "We train a simple MNIST model and export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Export to SavedModel format\n",
    "import shutil\n",
    "model_path = \"my_mnist_model/0001\" # Version number 1\n",
    "shutil.rmtree(model_path, ignore_errors=True)\n",
    "tf.saved_model.save(model, model_path)\n",
    "\n",
    "print(\"Model saved to:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inspecting the Saved Model\n",
    "We use the command line tool `saved_model_cli` to inspect the inputs/outputs of the exported graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {model_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TensorFlow Serving (Simulation)\n",
    "In a real environment, you would run the docker container:\n",
    "```bash\n",
    "docker run -p 8501:8501 -v \"$PWD/my_mnist_model:/models/my_mnist_model\" -e MODEL_NAME=my_mnist_model tensorflow/serving\n",
    "```\n",
    "Since we are in a notebook, we will simulate the REST API call that a client would make to this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Prepare data payload (JSON)\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_test[:3].tolist()\n",
    "})\n",
    "\n",
    "# In real life, you use requests.post('http://localhost:8501/...', data=input_data_json)\n",
    "# Here we just show what the payload looks like\n",
    "print(\"JSON Payload (truncated):\", input_data_json[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 TensorFlow Lite Converter\n",
    "We convert the SavedModel to TFLite format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "\n",
    "# Simple conversion\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "print(\"TFLite model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 TFLite Quantization (Optimization)\n",
    "We optimize the model size by reducing precision to float16 or int8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open(\"model_quantized.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"Quantized TFLite model saved. Size comparison:\")\n",
    "print(\"Original:\", len(tflite_model), \"bytes\")\n",
    "print(\"Quantized:\", len(tflite_quant_model), \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Distributed Training (MirroredStrategy)\n",
    "This code snippet shows how to modify your training code to run on multiple GPUs. It is shockingly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "# Define the strategy\n",
    "# If you have only 1 GPU (or CPU), this still works (it just mirrors to 1 device)\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"Number of devices:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Open a scope. Everything created inside is 'distributed aware'\n",
    "with strategy.scope():\n",
    "    model_dist = create_model()\n",
    "    model_dist.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "# Train as usual. TF handles the data splitting and gradient aggregation.\n",
    "model_dist.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. SavedModel Structure\n",
    "The directory `0001` (version 1) is important. TF Serving automatically watches the root directory. If you export a new model to `0002`, Serving will detect it and switch traffic to the new version seamlessly without downtime.\n",
    "\n",
    "### 2. TFLite Conversion\n",
    "The converter acts like a compiler. It takes the TensorFlow Graph (designed for training, with backward pass capabilities) and freezes it into a flat, optimized FlatBuffer (designed purely for forward pass inference). It fuses operations (e.g., Conv + ReLU become one op) to save time.\n",
    "\n",
    "### 3. Mirrored Strategy Magic\n",
    "When you call `model.fit()` inside the `strategy.scope()`:\n",
    "1.  TF detects you have N GPUs.\n",
    "2.  It creates a copy of the model variables (weights) on each GPU.\n",
    "3.  It slices your input batch (say, 32 images) into $32/N$ sub-batches.\n",
    "4.  Each GPU calculates gradients for its sub-batch.\n",
    "5.  **All-Reduce:** A highly optimized algorithm (like NVIDIA NCCL) sums the gradients from all GPUs and broadcasts the total back to everyone.\n",
    "6.  Every GPU applies the same update, keeping the weights in sync."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Exporting:** Always use `SavedModel` format for production.\n",
    "* **Serving:** Use TensorFlow Serving for server-side deployment. It handles versioning and batching automatically.\n",
    "* **Mobile:** Use TensorFlow Lite for Android/iOS/IoT. Use Quantization to reduce model size by 4x.\n",
    "* **Scaling:** Use `MirroredStrategy` to train on multiple GPUs. It requires almost zero code changes.\n",
    "\n",
    "---\n",
    "\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
