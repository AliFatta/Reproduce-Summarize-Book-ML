{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Unsupervised Learning Techniques\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** In previous chapters, we dealt with Supervised Learning (we had labels). Now, we explore **Unsupervised Learning**, where the data is unlabeled. We want the algorithm to discover hidden structures, patterns, or anomalies in the data on its own.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Clustering:** Grouping similar instances together.\n",
    "    * **K-Means:** The most popular, centroid-based algorithm.\n",
    "    * **DBSCAN:** Density-based algorithm that handles arbitrary shapes.\n",
    "* **Clustering Evaluation:** The Elbow Method and Silhouette Score.\n",
    "* **Image Segmentation:** Using clustering to reduce colors in an image.\n",
    "* **Gaussian Mixtures (GMM):** A probabilistic model that assumes data is generated from a mixture of Gaussian distributions.\n",
    "* **Anomaly Detection:** Using GMM to detect outliers (e.g., fraud detection).\n",
    "\n",
    "**Practical Skills:**\n",
    "* Implementing K-Means and optimizing `k`.\n",
    "* Visualizing decision boundaries (Voronoi tessellation).\n",
    "* Performing color segmentation on images.\n",
    "* Using Gaussian Mixtures for density estimation and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. K-Means Clustering\n",
    "K-Means is a simple and efficient algorithm. The idea is to find $k$ centroids (center points) and assign every data point to the nearest centroid. \n",
    "\n",
    "**The Algorithm Steps:**\n",
    "1.  **Initialize:** Place $k$ centroids randomly.\n",
    "2.  **Assign:** For each instance, calculate the distance to all centroids and assign it to the closest cluster.\n",
    "3.  **Update:** Move the centroids to the *mean* (center of mass) of the instances assigned to them.\n",
    "4.  **Repeat:** Repeat steps 2 and 3 until the centroids stop moving (convergence).\n",
    "\n",
    "**Hard vs. Soft Clustering:**\n",
    "* *Hard Clustering:* Each instance belongs to exactly one cluster.\n",
    "* *Soft Clustering:* Each instance has a score/probability for each cluster (e.g., distance to centroid).\n",
    "\n",
    "**Limitations:**\n",
    "* You must choose $k$ in advance.\n",
    "* It assumes clusters are roughly spherical and of similar size. It struggles with elongated blobs or irregular shapes.\n",
    "\n",
    "### 2. Selecting the Optimal Number of Clusters\n",
    "Since we don't have labels, how do we know if 3 clusters is better than 5?\n",
    "\n",
    "* **Inertia:** The sum of squared distances between instances and their closest centroid. Lower is better, BUT inertia always decreases as $k$ increases (if $k=$ number of instances, inertia is 0). We look for an **Elbow** point where the decrease slows down.\n",
    "* **Silhouette Score:** Measures how close an instance is to its own cluster compared to the nearest neighboring cluster. Range is -1 to +1. +1 means well-clustered, 0 means overlapping, -1 means wrong cluster.\n",
    "\n",
    "### 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "DBSCAN defines clusters as continuous regions of high density. \n",
    "* **Core Instance:** An instance that has at least `min_samples` neighbors within a distance `epsilon`.\n",
    "* **Border Instance:** An instance close to a core instance but not core itself.\n",
    "* **Noise/Outlier:** An instance that is neither core nor border.\n",
    "\n",
    "**Advantages:** It can find clusters of *any shape* (unlike K-Means which likes spheres) and automatically detects outliers.\n",
    "\n",
    "### 4. Gaussian Mixture Models (GMM)\n",
    "A GMM is a probabilistic model that assumes the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. Unlike K-Means (which gives hard circles), GMM gives soft ellipses.\n",
    "* It uses the **Expectation-Maximization (EM)** algorithm to find the parameters (Mean and Covariance matrix) of the Gaussians.\n",
    "* **Anomaly Detection:** Any instance located in a low-density region (low probability) can be considered an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 K-Means on Blobs\n",
    "We generate synthetic \"blob\" data to visualize how K-Means works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate blobs\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "# Train K-Means with k=5\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Check the centroids found\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n",
    "\n",
    "# Visualization\n",
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X, y_pred)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='red', marker='x')\n",
    "plt.title(\"K-Means Clustering (k=5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Finding the Optimal k (Elbow Method & Silhouette)\n",
    "Let's see what happens if we don't know there are 5 blobs. We test k from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "\n",
    "# Inertias\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "# Silhouette Scores (starts from k=2)\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow', xy=(4, inertias[3]), xytext=(4.5, 650),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1))\n",
    "plt.title(\"Inertia (Elbow Method)\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette Score\", fontsize=14)\n",
    "plt.title(\"Silhouette Score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Image Segmentation\n",
    "We will load a sample image (flower), and use K-Means to cluster the colors. By replacing each pixel's color with the cluster's mean color, we segment the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "# Load sample images\n",
    "images = load_sample_images() \n",
    "img = images.images[1]  # The flower image\n",
    "\n",
    "# Reshape image to a list of RGB colors (pixels)\n",
    "X_img = img.reshape(-1, 3)\n",
    "\n",
    "# Segment into 4 colors\n",
    "kmeans = KMeans(n_clusters=4, random_state=42).fit(X_img)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(img.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "# Cast to uint8 for valid display\n",
    "plt.imshow(segmented_img.astype(np.uint8))\n",
    "plt.title(\"Segmented (4 Colors)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 DBSCAN\n",
    "We use the Moons dataset (which K-Means fails on) to show the power of density-based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Check how many clusters found (label -1 is noise)\n",
    "print(\"Labels:\", np.unique(dbscan.labels_))\n",
    "\n",
    "plot_clusters(X, dbscan.labels_)\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Gaussian Mixture & Anomaly Detection\n",
    "We will fit a GMM to the blob data and then calculate the density. Instances in low-density regions (e.g., density < 4%) are flagged as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Use the blob data again\n",
    "X, y = make_blobs(n_samples=1000, centers=blob_centers, cluster_std=blob_std, random_state=42)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, n_init=10, random_state=42)\n",
    "gm.fit(X)\n",
    "\n",
    "# Score samples gives the log of the probability density function (PDF)\n",
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 4) # Bottom 4%\n",
    "anomalies = X[densities < density_threshold]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*', s=100, label='Anomalies')\n",
    "plt.title(\"GMM Anomaly Detection\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. K-Means Elbow & Silhouette\n",
    "**Input:** A range of cluster counts ($k=1$ to $9$).\n",
    "**Analysis:** \n",
    "* **Inertia Plot:** We see a sharp drop from $k=1$ to $k=3$, and a noticeable bend at $k=4$ or $k=5$. After $k=5$, inertia goes down very slowly. This suggests $k=4$ or $5$ is optimal.\n",
    "* **Silhouette Plot:** The score is highest at $k=4$ and $k=5$, confirming our observation.\n",
    "\n",
    "### 2. Image Segmentation Logic\n",
    "**Input:** A 3D array (height, width, RGB channels).\n",
    "**Process:**\n",
    "1.  Flatten the image into a long list of pixels (ignoring position, looking only at color).\n",
    "2.  Run K-Means with $k=4$. It finds the 4 dominant colors (centers).\n",
    "3.  Replace every pixel with its nearest dominant color.\n",
    "**Output:** An image composed of only 4 colors. This significantly reduces file size and complexity, useful for object detection systems.\n",
    "\n",
    "### 3. DBSCAN vs. K-Means\n",
    "* K-Means would have failed on the Moons dataset; it would have drawn a straight line through the middle.\n",
    "* DBSCAN connects points that are close together. Since the two moons are dense regions separated by a gap (low density), DBSCAN correctly identifies them as two separate shapes.\n",
    "\n",
    "### 4. Anomaly Detection\n",
    "**Concept:** If a data point is very far from any cluster center (centroid), its probability density is low.\n",
    "**Process:** We fit the Gaussian Mixture. We calculate the likelihood of every point. We define a threshold (e.g., \"the lowest 4%\").\n",
    "**Output:** The red stars in the plot are the anomalies. In a manufacturing context, these could be defective products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Clustering** allows us to segment data without labels.\n",
    "* **K-Means:** Fast, scalable, but assumes spherical clusters. Requires picking $k$.\n",
    "* **DBSCAN:** Great for arbitrary shapes and outlier detection. Parameters `eps` and `min_samples` control density sensitivity.\n",
    "* **GMM (Gaussian Mixtures):** Generalizes K-Means to ellipsoidal shapes and provides probabilistic assignments (soft clustering).\n",
    "* **Anomaly Detection:** Unsupervised learning is excellent for finding outliers (fraud, defects) by looking for instances in low-density regions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
