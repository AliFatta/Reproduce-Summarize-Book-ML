{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Dimensionality Reduction\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Many Machine Learning problems involve thousands or even millions of features for each training instance. This makes training extremely slow and makes it harder to find a good solution. This problem is often referred to as the **Curse of Dimensionality**. In this chapter, we explore techniques to reduce the number of features (dimensions) while losing as little information as possible.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **The Curse of Dimensionality:** Why high-dimensional space is sparse and dangerous.\n",
    "* **Approaches:** Projection vs. Manifold Learning.\n",
    "* **PCA (Principal Component Analysis):** Preserving variance, SVD, and reconstruction error.\n",
    "* **Choosing the Right Dimensions:** The \"Elbow\" method and 95% variance rule.\n",
    "* **PCA Variations:** Randomized PCA, Incremental PCA (for large datasets).\n",
    "* **Kernel PCA:** Performing complex nonlinear projections.\n",
    "* **LLE (Locally Linear Embedding):** A powerful manifold learning technique.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Implementing PCA using Scikit-Learn to compress data.\n",
    "* Visualizing high-dimensional data in 2D or 3D.\n",
    "* Tuning hyperparameters for Kernel PCA.\n",
    "* Using LLE to unroll a \"Swiss Roll\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. The Curse of Dimensionality\n",
    "We are used to living in 3 dimensions. It is very hard to intuit what happens in 1,000 dimensions. High-dimensional spaces behave very differently from the 2D/3D space we know.\n",
    "\n",
    "**Sparseness:**\n",
    "If you pick a random point in a unit square (2D), there is a 0.04% chance it is located near the border (extreme value). But in a 10,000-dimensional hypercube, the probability is greater than 99.999999%. This means high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. \n",
    "\n",
    "**Distance & Overfitting:**\n",
    "Because instances are far apart, predictions become much less reliable than in lower dimensions, and the model is prone to **overfitting**. Theoretically, you could solve this by adding more training data to fill the gaps, but the number of instances required grows exponentially with the number of dimensions.\n",
    "\n",
    "### 2. Main Approaches to Reduction\n",
    "\n",
    "**A. Projection:**\n",
    "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are constant, while others are highly correlated. As a result, all training instances actually lie within (or close to) a much lower-dimensional *subspace*.\n",
    "* *Analogy:* Imagine a 3D object (like a chair) casting a shadow on a 2D wall. If you deal with the shadow (2D), you lose some information (depth), but you keep the general shape. This is projection.\n",
    "\n",
    "**B. Manifold Learning:**\n",
    "Sometimes, projection is not enough. Consider the **Swiss Roll** dataset (a 2D plane rolled up into a 3D spiral). If you project this onto a 2D plane (squash it), layers will overlap and mix the colors/classes. What you really want to do is *unroll* the Swiss Roll.\n",
    "* A **d-dimensional manifold** is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. \n",
    "* **Manifold Learning** relies on the assumption that real-world high-dimensional datasets actually lie close to a much lower-dimensional manifold.\n",
    "\n",
    "### 3. PCA (Principal Component Analysis)\n",
    "PCA is by far the most popular dimensionality reduction algorithm. It identifies the hyperplane that lies closest to the data and then projects the data onto it.\n",
    "\n",
    "**Preserving Variance:**\n",
    "Before projecting, you need to choose the right hyperplane. PCA selects the axis that preserves the maximum amount of variance (information). \n",
    "* If you project a long oval onto its short axis, you lose a lot of spread (variance). \n",
    "* If you project it onto its long axis, you keep most of the spread. \n",
    "PCA finds the axis that minimizes the mean squared distance between the original dataset and its projection.\n",
    "\n",
    "**Principal Components:**\n",
    "PCA finds the first axis (PC1) that accounts for the largest amount of variance. Then it finds a second axis (PC2), orthogonal to the first, that accounts for the largest amount of remaining variance, and so on.\n",
    "\n",
    "**SVD (Singular Value Decomposition):**\n",
    "How does PCA find these axes mathematically? It uses a standard matrix factorization technique called SVD. It can decompose the training set matrix $X$ into the dot product of three matrices: $U \\cdot \\Sigma \\cdot V^T$. \n",
    "The matrix $V^T$ contains the unit vectors that define all the Principal Components.\n",
    "\n",
    "### 4. Variations of PCA\n",
    "\n",
    "**Randomized PCA:**\n",
    "Instead of using the full SVD algorithm (which is slow: $O(m \\times n^2) + O(n^3)$), Scikit-Learn uses a stochastic algorithm called Randomized PCA. It quickly finds an approximation of the first $d$ principal components. Its complexity is drastically lower: $O(m \\times d^2) + O(d^3)$.\n",
    "\n",
    "**Incremental PCA (IPCA):**\n",
    "Standard PCA requires the whole dataset to fit in memory. For huge datasets, IPCA allows you to split the training set into mini-batches and feed the algorithm one mini-batch at a time.\n",
    "\n",
    "**Kernel PCA (kPCA):**\n",
    "Just like with SVMs, we can use the Kernel Trick to implicitly map instances into a very high-dimensional feature space (where they become linearly separable) and then perform PCA in that space. This is great for unrolling nonlinear datasets like the Swiss Roll.\n",
    "\n",
    "### 5. LLE (Locally Linear Embedding)\n",
    "LLE is a Manifold Learning technique that does not rely on projections. \n",
    "1. For each training instance $x^{(i)}$, it identifies its $k$ nearest neighbors.\n",
    "2. It tries to reconstruct $x^{(i)}$ as a linear function of these neighbors.\n",
    "3. It then finds a low-dimensional representation of the data that preserves these local relationships as much as possible.\n",
    "LLE is very good at unrolling twisted manifolds where PCA would fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 PCA for Data Compression\n",
    "We will create a simple 3D dataset that is actually flat (shaped like a pancake) and project it down to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 3D dataset\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "print(\"Reduced shape:\", X2D.shape)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `explained_variance_ratio_` tells us that the first dimension carries e.g., 84% of the dataset's variance, and the second carries 14%. We lost less than 2% of the information by dropping the 3rd dimension.\n",
    "\n",
    "### 3.2 Choosing the Right Number of Dimensions (MNIST)\n",
    "Instead of guessing `n_components`, we can plot the cumulative explained variance as a function of the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Fetch MNIST (this might take a while)\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "\n",
    "# Compute PCA without reducing dimensionality yet to see full variance\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "print(f\"Dimensions required to preserve 95% variance: {d}\")\n",
    "\n",
    "# Plotting the Elbow Curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Explained Variance vs Dimensions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Compression\n",
    "We can compress the dataset to 154 dimensions (from 784) and then decompress it back to 784. The result won't be identical, but it should be very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "# Plotting Original vs Compressed\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Kernel PCA (Non-linear)\n",
    "We use the Swiss Roll dataset to demonstrate how Kernel PCA can unroll nonlinear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X_swiss)\n",
    "\n",
    "# Visualizing the unrolled Swiss Roll\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.title(\"Kernel PCA (RBF) Unrolling\")\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LLE (Locally Linear Embedding)\n",
    "LLE works by first measuring how each training instance linearly relates to its closest neighbors (c), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_reduced_lle = lle.fit_transform(X_swiss)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced_lle[:, 0], X_reduced_lle[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.title(\"Locally Linear Embedding (LLE)\")\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Elbow Plot Analysis\n",
    "**Input:** The cumulative sum of the explained variance ratio from standard PCA.\n",
    "**Process:** As we add dimensions (x-axis), the total variance explained (y-axis) increases. It rises sharply at first and then flattens out.\n",
    "**Output:** We see an \"elbow\" around 154 dimensions where the curve reaches 95%. This means the other 600+ pixels in the MNIST images are mostly noise or redundant border pixels and can be safely discarded.\n",
    "\n",
    "### 2. Compression/Decompression\n",
    "**Concept:** $X_{recovered} = X_{d-proj} \\cdot V^T$\n",
    "* When we reduce data, we lose information (compression loss).\n",
    "* When we reconstruct it (`inverse_transform`), we map the 154 values back to 784 pixels.\n",
    "* **Visual Check:** The compressed digits look slightly blurry (missing high-frequency details) but are perfectly recognizable. This proves PCA captured the structural essence of the digits.\n",
    "\n",
    "### 3. Kernel PCA vs. LLE\n",
    "* **Kernel PCA (RBF):** It used a similarity function to separate the rolled layers implicitly. The result is a 2D projection that looks somewhat unrolled.\n",
    "* **LLE:** It looked at the neighbors of each point on the Swiss Roll. It noticed that points are flat locally. By preserving these neighbor distances while flattening, it successfully unrolled the Swiss Roll into a clean 2D strip, often better than PCA for manifolds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Dimensionality Reduction** speeds up training, removes noise, and helps visualize data.\n",
    "* **PCA:** The gold standard for linear projection. It rotates the data to align with the axes of highest variance.\n",
    "* **SVD:** The linear algebra engine under PCA's hood.\n",
    "* **Choosing Dimensions:** Use `n_components` as a float (e.g., 0.95) to preserve a % of variance, or look for the elbow in the cumulative variance plot.\n",
    "* **Randomized PCA:** Use this for faster training on large datasets.\n",
    "* **Kernel PCA:** Use this for complex nonlinear datasets (like spirals).\n",
    "* **Manifold Learning (LLE, t-SNE):** Best for visualization and highly twisted datasets where simple projection destroys the structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
