{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: End-to-End Machine Learning Project\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** In this chapter, we will work through a complete Machine Learning project from start to finish. We will pretend to be a data scientist in a real estate company. Our task is to predict median house values in Californian districts, given a number of features from these districts.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Big Picture:** Understanding the problem (Supervised, Regression, Batch).\n",
    "* **Data Splitting:** Creating a test set and stratified sampling.\n",
    "* **Data Visualization:** Gaining insights from geographical data.\n",
    "* **Data Preparation:** Handling missing values, categorical features, and feature scaling.\n",
    "* **Transformation Pipelines:** Automating data processing.\n",
    "* **Model Selection:** Training and evaluating multiple models (Linear Regression, Decision Tree, Random Forest).\n",
    "* **Fine-Tuning:** Using Grid Search to find the best hyperparameters.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Using `Scikit-Learn` pipelines (`Pipeline`, `ColumnTransformer`).\n",
    "* Data cleaning with `SimpleImputer`.\n",
    "* Encoding text data with `OneHotEncoder`.\n",
    "* Cross-Validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures directly within Jupyter\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation\n",
    "\n",
    "### 1. Problem Definition\n",
    "* **Supervised Learning:** We have labeled training data (the median house value is known for each district).\n",
    "* **Regression Task:** We are predicting a continuous value (price).\n",
    "    * *Multiple Regression:* We use multiple features (population, median income, etc.) to make a prediction.\n",
    "    * *Univariate Regression:* We are predicting a single value per district.\n",
    "* **Batch Learning:** The system is trained on all available data offline, not incrementally.\n",
    "\n",
    "### 2. Performance Measures\n",
    "To evaluate how good our regression model is, we typically use:\n",
    "\n",
    "**RMSE (Root Mean Square Error):**\n",
    "$$ RMSE(X, h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 } $$\n",
    "* It gives more weight to large errors.\n",
    "* Standard metric for regression.\n",
    "\n",
    "**MAE (Mean Absolute Error):**\n",
    "$$ MAE(X, h) = \\frac{1}{m} \\sum_{i=1}^{m} |h(x^{(i)}) - y^{(i)}| $$\n",
    "* Preferred if there are many outliers in the data.\n",
    "\n",
    "### 3. Stratified Sampling\n",
    "Random sampling works for large datasets, but for smaller ones, you risk **sampling bias**. Stratified sampling ensures that the test set is representative of the overall population by dividing the population into homogeneous subgroups (strata) and sampling from each stratum to match the overall distribution (e.g., ensuring income categories are represented proportionally).\n",
    "\n",
    "### 4. Pipelines\n",
    "In ML, data must go through a sequence of processing steps (imputation $\\rightarrow$ scaling $\\rightarrow$ modeling). A **Pipeline** ensures these steps are executed in the correct order and makes the code reproducible and deployable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "We will implement the project step-by-step as described in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Fetching and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Fetch and load\n",
    "fetch_housing_data()\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating a Test Set (Stratified Sampling)\n",
    "We create an income category attribute to perform stratified sampling, as median income is a very important attribute for predicting housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Create income categories to categorize the continuous 'median_income' variable\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# Drop the temporary income_cat column so data returns to original state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "print(\"Training set size:\", len(strat_train_set))\n",
    "print(\"Test set size:\", len(strat_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Discover and Visualize the Data\n",
    "We use the training set only for exploration to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()\n",
    "\n",
    "# Visualize geographical data\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.title(\"California Housing Prices\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Cleaning and Preparation Pipeline\n",
    "We will separate the predictors (features) from the labels (target). Then we build a pipeline to handle numerical and categorical attributes separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Pipeline for numerical attributes\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")), # Fill missing values with median\n",
    "    ('std_scaler', StandardScaler()),              # Scale features\n",
    "])\n",
    "\n",
    "# Full pipeline handling both numerical and categorical\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),         # Convert text categories to numbers\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "print(\"Data prepared shape:\", housing_prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Select and Train a Model\n",
    "We will train three models: Linear Regression, Decision Tree, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(f\"Linear Regression RMSE: {lin_rmse}\")\n",
    "\n",
    "# 2. Decision Tree Regressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(f\"Decision Tree RMSE: {tree_rmse}\") # Likely 0.0 due to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Evaluation using Cross-Validation\n",
    "The Decision Tree likely overfitted (0 error). We use Cross-Validation to get a better estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "# Cross-validation for Decision Tree\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\"\\nDecision Tree Cross-Validation:\")\n",
    "display_scores(tree_rmse_scores)\n",
    "\n",
    "# Cross-validation for Random Forest\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "\n",
    "print(\"\\nRandom Forest Cross-Validation:\")\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Fine-Tuning the Model (Grid Search)\n",
    "We use GridSearchCV to find the best combination of hyperparameters for the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate final model on test set\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print(\"Final RMSE on Test Set:\", final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Data Pipeline (`full_pipeline`)\n",
    "**Input**: Raw pandas DataFrame.\n",
    "**Process**:\n",
    "1.  **Imputation**: The `SimpleImputer` replaces `NaN` values in numerical columns with the column median. This prevents errors during mathematical operations.\n",
    "2.  **Scaling**: The `StandardScaler` standardizes features (mean=0, variance=1). This is crucial for algorithms like Linear Regression and Neural Networks, which converge faster with scaled data.\n",
    "3.  **Encoding**: The `OneHotEncoder` transforms the categorical `ocean_proximity` (e.g., \"INLAND\", \"NEAR BAY\") into sparse binary vectors (dummy variables).\n",
    "**Output**: A clean, numerical NumPy array ready for training.\n",
    "\n",
    "### 2. Model Training & Evaluation\n",
    "* **Linear Regression**: Fitted a straight line. The RMSE was high (underfitting), meaning the model is too simple for the complex data.\n",
    "* **Decision Tree**: Achieved 0.0 RMSE on training data but performed poorly on cross-validation. This is a classic example of **overfitting**; it memorized the training data.\n",
    "* **Random Forest**: An ensemble of Decision Trees. It performed significantly better because it averages out the predictions of many trees, reducing overfitting.\n",
    "\n",
    "### 3. Grid Search\n",
    "Instead of manually guessing hyperparameters (like `n_estimators` for the number of trees), `GridSearchCV` tries every combination defined in `param_grid`. It uses Cross-Validation for each combination to ensure the selected parameters generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **End-to-End Workflow**: We covered the entire process: fetching data, cleaning it, choosing a model, tuning it, and evaluating it.\n",
    "* **Data Exploration**: Visualization reveals patterns (like location clustering) that can inform feature engineering.\n",
    "* **Preprocessing is Key**: Real-world data is messy. Pipelines handles missing values and categorical data systematically.\n",
    "* **Evaluation**: RMSE is the standard metric for regression. Cross-validation provides a more reliable error estimate than a single validation set.\n",
    "* **Automation**: `GridSearchCV` automates the tedious process of hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
