{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Models\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Up to this point, we treated Machine Learning models as \"black boxes\". In this chapter, we will open the box to understand how they work under the hood. Understanding these details helps in quickly narrowing down the right model, the right training algorithm, and the right hyperparameters.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Linear Regression:** The Normal Equation and Computational Complexity.\n",
    "* **Gradient Descent:** Batch, Stochastic, and Mini-batch GD.\n",
    "* **Polynomial Regression:** Fitting complex data with linear models.\n",
    "* **Learning Curves:** analyzing overfitting and underfitting.\n",
    "* **Regularized Linear Models:** Ridge, Lasso, and Elastic Net.\n",
    "* **Logistic Regression:** Using regression for classification (estimating probabilities).\n",
    "\n",
    "**Practical Skills:**\n",
    "* Training models using the Normal Equation (`numpy.linalg`).\n",
    "* Using `PolynomialFeatures` to transform data.\n",
    "* Plotting Learning Curves to diagnose model performance.\n",
    "* Implementing Logistic and Softmax Regression on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation\n",
    "\n",
    "### 1. Linear Regression\n",
    "The simplest model. It makes a prediction by computing a weighted sum of the input features, plus a constant bias term (intercept).\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n $$\n",
    "\n",
    "**How to train it?**\n",
    "We need to find the value of $\\theta$ that minimizes the Mean Squared Error (MSE). There is a closed-form mathematical solution called the **Normal Equation**:\n",
    "$$ \\hat{\\theta} = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "### 2. Gradient Descent\n",
    "The Normal Equation gets very slow when the number of features is large. Gradient Descent is an optimization algorithm capable of finding optimal solutions for a wide range of problems. The general idea is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "* **Batch GD:** Uses the whole training set at every step (slow on large data).\n",
    "* **Stochastic GD:** Picks a random instance at every step (fast but irregular).\n",
    "* **Mini-batch GD:** Uses small random sets of instances (compromise).\n",
    "\n",
    "### 3. Bias/Variance Tradeoff\n",
    "* **Bias:** Error due to wrong assumptions (e.g., assuming data is linear when it is quadratic). Leads to underfitting.\n",
    "* **Variance:** Error due to model sensitivity to small variations in training data. Leads to overfitting.\n",
    "\n",
    "### 4. Regularization\n",
    "Constraining a model to reduce overfitting.\n",
    "* **Ridge (L2):** Adds \"squared magnitude\" of coefficients as penalty term to the loss function.\n",
    "* **Lasso (L1):** Adds \"absolute value\" of coefficients. Can completely eliminate least important features (feature selection).\n",
    "* **Elastic Net:** A mix of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Linear Regression using the Normal Equation\n",
    "Let's generate some linear-looking data to test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Normal Equation manually using NumPy\n",
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(\"Best Theta calculated:\\n\", theta_best)\n",
    "\n",
    "# Make predictions using the calculated theta\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Polynomial Regression\n",
    "What if the data is not linear? We can use a linear model to fit nonlinear data by adding powers of each feature as new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate quadratic data\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Transform training data: adds the square of each feature\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Fit Linear Regression on transformed data\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "# Plotting\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learning Curves\n",
    "Learning curves are plots of the model's performance on the training set and the validation set as a function of the training set size. They help detect underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    \n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.legend()\n",
    "\n",
    "# Plot for a simple Linear Regression (Underfitting)\n",
    "plot_learning_curves(LinearRegression(), X, y)\n",
    "plt.title(\"Underfitting Example (Linear Model on Quadratic Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Logistic Regression (Classification)\n",
    "Logistic regression estimates the probability that an instance belongs to a particular class. If the estimated probability is > 50%, the model predicts that the instance belongs to that class.\n",
    "\n",
    "We use the **Iris dataset** to detect the \"Iris-Virginica\" type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Visualize the probabilities\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\n",
    "plt.xlabel(\"Petal width (cm)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Normal Equation\n",
    "**Input:** A matrix `X_b` containing our features (and the bias term of 1s).\n",
    "**Process:** We perform matrix multiplication and inversion: $(X^T X)^{-1} X^T y$. This is an exact mathematical solution.\n",
    "**Output:** The optimal $\\theta$ values that minimize the error. For our synthetic data $y = 4 + 3x$, the result should be close to 4 (intercept) and 3 (slope).\n",
    "\n",
    "### 2. Polynomial Features\n",
    "**Problem:** A straight line cannot fit a curve (parabola).\n",
    "**Solution:** We square the existing feature $x$ to create $x^2$. Now the linear regression model sees two features ($x$ and $x^2$) and learns a formula like $y = \\theta_0 + \\theta_1 x + \\theta_2 x^2$. This allows it to fit quadratic data while still being a \"Linear Regression\" algorithm mathematically.\n",
    "\n",
    "### 3. Learning Curves Interpretation\n",
    "* **Underfitting:** Both training and validation errors are high and reach a plateau. Adding more data won't help.\n",
    "* **Overfitting:** Training error is low, but validation error is high (gap between curves). The model is too complex.\n",
    "\n",
    "### 4. Logistic Regression Decision Boundary\n",
    "In the plot, the decision boundary is where the probability lines cross (at 50%). If the petal width is greater than approx 1.6 cm, the classifier predicts \"Iris-Virginica\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Iterative Optimization:** Gradient Descent (and its variants) is the engine behind most ML models (especially Neural Networks). It tweaks parameters step-by-step to minimize error.\n",
    "* **Polynomial Regression:** A powerful trick to add complexity to simple linear models.\n",
    "* **Diagnostics:** Learning curves are essential tools to diagnose if your model needs more data, more features, or regularization.\n",
    "* **Regularization:** Techniques like Ridge and Lasso are critical to prevent complex models from memorizing noise (overfitting).\n",
    "* **Logistic Regression:** Despite the name \"Regression\", it is a classification algorithm used to estimate the probability of binary outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
