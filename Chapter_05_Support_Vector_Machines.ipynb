{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Support Vector Machines\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Understand Support Vector Machines (SVMs), a powerful and versatile Machine Learning model capable of performing linear or nonlinear classification, regression, and even outlier detection.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Large Margin Classification:** The core idea of fitting the \"widest possible street\" between classes.\n",
    "* **Hard vs. Soft Margin:** Balancing perfectly separating data vs. allowing some violations to generalize better.\n",
    "* **Kernel Trick:** A mathematical technique to map data into higher dimensions without actually calculating the coordinates, allowing for nonlinear classification.\n",
    "* **SVM Regression:** Reversing the objective (trying to fit as many instances *on* the street as possible).\n",
    "\n",
    "**Practical Skills:**\n",
    "* Using `LinearSVC` for fast linear classification.\n",
    "* Implementing `StandardScaler` (crucial for SVMs).\n",
    "* Using `SVC` with Polynomial and RBF kernels.\n",
    "* Tuning hyperparameters `C` and `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation\n",
    "\n",
    "### 1. Large Margin Classification\n",
    "Think of an SVM classifier as fitting the widest possible street (represented by the dashed lines) between the classes. This is called **Large Margin Classification**. New instances that fall off the street will not affect the decision boundary; the boundary is determined fully by the instances located on the edge of the street. These instances are called **Support Vectors**.\n",
    "\n",
    "### 2. Sensitivity to Feature Scaling\n",
    "SVMs are very sensitive to the scales of the features. If one feature has a much larger range than another (e.g., salary vs. age), the \"street\" will be almost parallel to the larger feature. **Always use Feature Scaling (StandardScaler)** before training an SVM.\n",
    "\n",
    "### 3. Hard Margin vs. Soft Margin\n",
    "* **Hard Margin:** Strictly imposes that all instances must be off the street and on the right side. Only works if data is linearly separable and is sensitive to outliers.\n",
    "* **Soft Margin:** Allows some instances to end up on the street or on the wrong side (margin violations) to keep the street wide and generalize better. This balance is controlled by the hyperparameter **C**.\n",
    "    * **High C:** Fewer margin violations, smaller margin (strict).\n",
    "    * **Low C:** More margin violations, wider margin (tolerant/regularized).\n",
    "\n",
    "### 4. The Kernel Trick\n",
    "To handle nonlinear data (like the Moons dataset), we can add polynomial features ($x^2, x^3$). However, adding many features slows down training. The **Kernel Trick** yields the same result as if you added many polynomial features, without actually adding them. It computes the dot product in a higher-dimensional space mathematically.\n",
    "\n",
    "* **Polynomial Kernel:** Simulates adding polynomial features.\n",
    "* **RBF (Radial Basis Function) Kernel:** Simulates adding \"similarity features\" (like Gaussian landmarks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Linear SVM Classification\n",
    "We use the Iris dataset again, specifically to detect Iris-Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "print(\"Prediction for [5.5, 1.7]:\", svm_clf.predict([[5.5, 1.7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Nonlinear SVM Classification\n",
    "For nonlinear data, we use the `make_moons` dataset (two interleaving half circles). We will use a Pipeline that adds polynomial features, scales them, and then applies a Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "# Visualization function\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"Linear SVM with Polynomial Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Polynomial Kernel (Kernel Trick)\n",
    "Instead of manually adding features (which explodes combinatorially), we use `SVC(kernel=\"poly\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Degree=3, coef0=1 controls how much the model is influenced by high-degree polynomials\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"SVM with Polynomial Kernel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Gaussian RBF Kernel\n",
    "This is the most popular kernel for general nonlinear problems. It adds features based on similarity to landmarks.\n",
    "\n",
    "$$ \\phi_\\gamma(\\mathbf{x}, \\ell) = \\exp(-\\gamma \\| \\mathbf{x} - \\ell \\|^2) $$\n",
    "\n",
    "* **Gamma ($\\gamma$):** Controls the bell-curve width. Higher gamma $\\rightarrow$ narrower bell curve $\\rightarrow$ more complex/irregular boundary (risk of overfitting).\n",
    "* **C:** Regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_predictions(rbf_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"SVM with RBF Kernel (gamma=5, C=0.001)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. The Pipeline\n",
    "You will notice `Pipeline` is used everywhere. This is standard practice for SVMs.\n",
    "1.  **StandardScaler:** SVM tries to maximize distance. If one axis is 100x larger than the other, the distance calculation is dominated by that axis. Scaling makes all axes contribute equally.\n",
    "2.  **Model:** `LinearSVC` or `SVC`.\n",
    "\n",
    "### 2. Linear vs. Kernel SVM\n",
    "* **`LinearSVC`:** Optimized for linear tasks. It does not support the kernel trick but scales well with the number of instances ($O(m \\times n)$).\n",
    "* **`SVC`:** Supports kernels (poly, rbf). It relies on libsvm. It is slower with large datasets ($O(m^2 \\times n)$ or worse) but powerful for complex small-medium datasets.\n",
    "\n",
    "### 3. Hyperparameter Tuning\n",
    "In the RBF example:\n",
    "* We used `gamma=5` (high gamma). This makes each instance's range of influence small. The decision boundary becomes jagged, trying to wrap around individual instances.\n",
    "* We used `C=0.001` (low C). This applies strong regularization, forcing a wider street even if it means misclassifying some training points. \n",
    "This combination likely results in **underfitting** in this specific visualization, showing a smooth \"blob\" rather than a tight fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **SVMs** classify data by finding the widest street (margin) between classes.\n",
    "* **Support Vectors** are the specific instances located on the edge of the street; they fully determine the decision boundary.\n",
    "* **Scaling is mandatory.** Always use `StandardScaler`.\n",
    "* **Kernel Trick:** Allows fitting nonlinear data efficiently without manually adding thousands of polynomial features.\n",
    "* **Hyperparameters:**\n",
    "    * **C:** Inverse of regularization. Small C = wide street (more violations). Large C = narrow street (strict).\n",
    "    * **Gamma:** (RBF Kernel only) Controls the range of influence. Small gamma = smooth boundary. Large gamma = irregular boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
