{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Decision Trees\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Understand Decision Trees, a versatile algorithm capable of performing both classification and regression tasks, and even multioutput tasks. They are powerful algorithms capable of fitting complex datasets and form the basis of Random Forests.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Training and Visualization:** How to build and interpret a tree.\n",
    "* **Making Predictions:** Traversing the tree from root to leaf.\n",
    "* **Gini Impurity vs. Entropy:** The math behind how the tree decides to split nodes.\n",
    "* **CART Algorithm:** The greedy algorithm used by Scikit-Learn to grow trees.\n",
    "* **Regularization:** Preventing overfitting using hyperparameters like `max_depth`.\n",
    "* **Regression:** Using trees to predict continuous values.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Training a `DecisionTreeClassifier` on the Iris dataset.\n",
    "* Visualizing the tree structure using `export_graphviz`.\n",
    "* Training a `DecisionTreeRegressor` on quadratic data.\n",
    "* Understanding the instability of trees (sensitivity to rotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation\n",
    "\n",
    "### 1. Structure of a Tree\n",
    "* **Root Node:** The top node where the tree starts.\n",
    "* **Split Node:** A node that checks a condition (e.g., \"Petal length <= 2.45 cm\") and splits the data.\n",
    "* **Leaf Node:** A terminal node that does not have children. It holds the final prediction.\n",
    "\n",
    "### 2. Gini Impurity\n",
    "A node's \"impurity\" measures how mixed the training instances are in that node. A node is \"pure\" (Gini = 0) if all instances it applies to belong to the same class.\n",
    "$$ G_i = 1 - \\sum_{k=1}^{n} p_{i,k}^2 $$\n",
    "Where $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i$-th node.\n",
    "\n",
    "### 3. The CART Algorithm\n",
    "Scikit-Learn uses the **Classification And Regression Tree (CART)** algorithm. It is a \"greedy\" algorithm: it searches for the single best pair of feature $k$ and threshold $t_k$ that produces the purest subsets (weighted by their size). It repeats this recursively until it reaches `max_depth` or cannot find a split that reduces impurity.\n",
    "\n",
    "### 4. White Box vs. Black Box\n",
    "* **White Box (Decision Trees):** Easy to interpret. You can visualize exactly why a decision was made.\n",
    "* **Black Box (Neural Networks, SVMs):** Often perform better, but hard to explain why specific predictions were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Training and Visualizing a Decision Tree\n",
    "We will train a tree on the Iris dataset to classify iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "# max_depth=2 restricts the tree depth to prevent overfitting\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "The book uses `export_graphviz`. This outputs a `.dot` file which can be converted to an image. \n",
    "*Note: This requires the Graphviz software to be installed on your system to render the PNG.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=\"iris_tree.dot\",\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "# To verify the file was created:\n",
    "print(\"iris_tree.dot file created successfully.\")\n",
    "\n",
    "# In a Jupyter environment with graphviz python package installed, you could render it like this:\n",
    "# import graphviz\n",
    "# with open(\"iris_tree.dot\") as f:\n",
    "#     dot_graph = f.read()\n",
    "# graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Estimating Class Probabilities\n",
    "A decision tree can estimate the probability that an instance belongs to a particular class by returning the ratio of training instances of that class in the leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating for a flower with petal length 5cm and width 1.5cm\n",
    "proba = tree_clf.predict_proba([[5, 1.5]])\n",
    "prediction = tree_clf.predict([[5, 1.5]])\n",
    "\n",
    "print(\"Probabilities (Setosa, Versicolor, Virginica):\", proba)\n",
    "print(\"Predicted Class Index:\", prediction)\n",
    "print(\"Predicted Class Name:\", iris.target_names[prediction][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Regularization Hyperparameters\n",
    "Decision Trees make very few assumptions about the training data (unlike Linear models). If left unconstrained, the tree structure will adapt itself to the training data perfectly, fitting it very closely â€” usually overfitting.\n",
    "\n",
    "To avoid this, we restrict the tree's freedom during training using:\n",
    "* `max_depth`: Maximum depth of the tree.\n",
    "* `min_samples_leaf`: Minimum samples a leaf node must have.\n",
    "\n",
    "Let's see the effect of regularization on the `moons` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "# 1. No Regularization (Default)\n",
    "deep_tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf.fit(Xm, ym)\n",
    "\n",
    "# 2. Regularized (min_samples_leaf=4)\n",
    "# This prevents the tree from creating leaf nodes with very few samples (likely noise)\n",
    "regularized_tree_clf = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "regularized_tree_clf.fit(Xm, ym)\n",
    "\n",
    "# Simple visualization of accuracy\n",
    "print(\"Depth of unregularized tree:\", deep_tree_clf.get_depth())\n",
    "print(\"Depth of regularized tree:\", regularized_tree_clf.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Regression Trees\n",
    "Decision Trees can also perform regression. Instead of predicting a class, it predicts a value (the average target value of the instances in that leaf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate noisy quadratic data\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10\n",
    "\n",
    "# Train a regression tree\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)\n",
    "\n",
    "# Visualization of the prediction line\n",
    "X_test = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_test, y_pred, \"r-\", linewidth=2, label=\"Prediction\")\n",
    "plt.title(\"Decision Tree Regression (max_depth=2)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Classification (Iris)\n",
    "**Process:** The `DecisionTreeClassifier` looks for the best feature to split the data. For example, it might find that `Petal Length <= 2.45` perfectly separates the *Setosa* flowers from the rest. It creates a node for this rule. Then it looks at the remaining data (Versicolor and Virginica) and finds the next best split (e.g., `Petal Width <= 1.75`).\n",
    "\n",
    "### 2. Prediction\n",
    "For a new flower with petal length 5cm:\n",
    "1. Start at root: Is length <= 2.45? No.\n",
    "2. Go to right child: Is width <= 1.75? Yes (assuming 1.5 < 1.75).\n",
    "3. Reach leaf node: This node contains mostly Versicolor.\n",
    "4. Output: Class Versicolor (and the probability is the % of Versicolor in that leaf).\n",
    "\n",
    "### 3. Regression\n",
    "**Input:** A single feature $X$.\n",
    "**Process:** The tree splits the $X$ axis into regions (e.g., $X < 0.2$, $0.2 \\le X < 0.8$, etc.).\n",
    "**Output:** For any new instance falling into a specific region, the tree predicts the **average** $y$-value of the training instances in that region. This results in the \"staircase\" (step function) appearance of the red line in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Interpretation:** Decision Trees are easy to understand and visualize.\n",
    "* **Data Prep:** They require very little data preparation (no scaling needed).\n",
    "* **Orthogonal Boundaries:** Trees make splits perpendicular to the feature axes. This makes them sensitive to data rotation (e.g., if you rotate the dataset 45 degrees, the staircase boundary becomes messy).\n",
    "* **Instability:** Trees are sensitive to small variations in the training data. A small change can result in a completely different tree structure. This is solved by averaging many trees (Random Forests).\n",
    "* **Overfitting:** Being non-parametric models, they love to overfit. Always use regularization parameters like `max_depth` or `min_samples_leaf`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
