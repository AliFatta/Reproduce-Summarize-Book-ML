{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Deep Learning systems often require massive amounts of data that cannot fit into RAM. Furthermore, if the CPU is slow at loading/processing data, the powerful GPU will sit idle, wasting resources. In this chapter, we master the **TensorFlow Data API (`tf.data`)** to build efficient, parallelized input pipelines. We also explore the **TFRecord** format for fast binary storage and **Keras Preprocessing Layers** to include preprocessing logic directly inside the model.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **The Data API (`tf.data`):** Creating, chaining, and optimizing dataset transformations.\n",
    "* **ETL Pipeline:** Extract (read), Transform (map/filter), Load (prefetch/batch).\n",
    "* **Performance Optimization:** Prefetching, caching, and parallelizing operations (`num_parallel_calls`).\n",
    "* **TFRecord Format:** A simple binary format for storing sequences of binary records (Protobufs).\n",
    "* **Protocol Buffers:** Defining structured data schemas for serialization.\n",
    "* **Keras Preprocessing Layers:** `Normalization`, `TextVectorization`, `CategoryEncoding`.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Creating datasets from tensors and CSV files.\n",
    "* Chaining `shuffle`, `batch`, `map`, and `prefetch` methods.\n",
    "* Writing and reading **TFRecord** files manually.\n",
    "* creating a custom `Example` protobuf message.\n",
    "* Building a model that accepts raw strings using the `TextVectorization` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. The Data API (`tf.data`)\n",
    "The Data API allows you to define a pipeline of operations. The core object is `tf.data.Dataset`. \n",
    "* **Immutability:** Datasets are immutable. Methods like `.map()` or `.batch()` do not modify the dataset; they return a new one.\n",
    "* **Lazy Evaluation:** Operations are not executed immediately. The dataset is just a graph of instructions. Data is only processed when you iterate over it (e.g., during training).\n",
    "\n",
    "**Key Transformations:**\n",
    "1.  **`from_tensor_slices()`:** Creates a dataset from a list/array in memory.\n",
    "2.  **`shuffle(buffer_size)`:** Fills a buffer with data and samples randomly from it. Crucial for SGD.\n",
    "3.  **`map(function)`:** Applies a transformation to each item (e.g., resizing images, parsing CSVs).\n",
    "4.  **`batch(size)`:** Groups items into batches.\n",
    "5.  **`prefetch(buffer_size)`:** The most important performance tool. While the GPU is training on batch $N$, the CPU prepares batch $N+1$ in parallel.\n",
    "\n",
    "### 2. The TFRecord Format\n",
    "CSV files are human-readable but inefficient (text parsing is slow). **TFRecord** is TensorFlow's preferred binary format. It stores a sequence of binary records. \n",
    "Each record is typically a serialized **Protocol Buffer** (protobuf). Protobuf is a portable, efficient, binary format developed by Google.\n",
    "\n",
    "**Structure:**\n",
    "* **`tf.train.Example`:** The standard protobuf message used in datasets. It contains a dictionary of **Features**.\n",
    "* **`tf.train.Features`:** A mapping of feature names to values (BytesList, FloatList, or Int64List).\n",
    "\n",
    "### 3. Keras Preprocessing Layers\n",
    "Historically, preprocessing (like scaling or one-hot encoding) was done in NumPy/Pandas before feeding data to the model. This creates a **Training-Serving Skew** risk (if the preprocessing code in production differs slightly from training).\n",
    "Keras now provides layers that handle preprocessing *inside* the model graph.\n",
    "* **`Normalization`:** Replaces StandardScaler.\n",
    "* **`TextVectorization`:** Handles tokenization and indexing for NLP.\n",
    "* **`CategoryEncoding`:** Handles one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Basic `tf.data` Pipeline\n",
    "We create a dataset from a simple range of numbers and apply a chain of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from a list of numbers 0 to 9\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Chain transformations\n",
    "dataset = dataset.repeat(3) # Repeat the dataset 3 times\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42)\n",
    "dataset = dataset.batch(7) # Group into batches of 7\n",
    "dataset = dataset.map(lambda x: x * 2) # Double every value\n",
    "\n",
    "# Iterate and inspect\n",
    "for batch in dataset:\n",
    "    print(batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Splitting the California Housing Dataset\n",
    "We will load the housing data, split it into multiple CSV files, and then build a pipeline to read them in parallel. This simulates handling a \"Big Data\" scenario where data is sharded across many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Function to save data into multiple CSV files\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]])) \n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "print(\"Created 20 training CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Building the Input Pipeline\n",
    "We use `list_files` to shuffle filenames, `interleave` to read from multiple files simultaneously, and `TextLineDataset` to read lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get file paths\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "# 2. Interleave: Read from 5 files at a time\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # skip header\n",
    "    cycle_length=n_readers)\n",
    "\n",
    "# 3. Parse CSV lines\n",
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_train.mean(axis=0)) / X_train.std(axis=0), y # simple scaling\n",
    "\n",
    "# 4. Apply transformations\n",
    "batch_size = 32\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(1) # Prefetch for GPU optimization\n",
    "\n",
    "print(\"Pipeline ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The TFRecord Format\n",
    "We will demonstrate how to write data to a TFRecord file and read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions to create Protobuf Features\n",
    "def bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() \n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# Create a dictionary representing one data instance (an Example)\n",
    "def create_example(house_idx, median_income, population):\n",
    "    feature = {\n",
    "        \"house_idx\": int64_feature(house_idx),\n",
    "        \"median_income\": float_feature(median_income),\n",
    "        \"population\": float_feature(population)\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "# Write to TFRecord file\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as writer:\n",
    "    for i in range(5):\n",
    "        example = create_example(i, np.random.rand(), np.random.rand() * 100)\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "print(\"Written my_data.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Reading and Parsing TFRecords\n",
    "Now we define a schema to parse the binary data back into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"house_idx\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"median_income\": tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "    \"population\": tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_data.tfrecord\"])\n",
    "dataset = dataset.map(parse_example)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Keras Preprocessing Layers: TextVectorization\n",
    "Preprocessing raw text directly within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Raw text data\n",
    "training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"], [\"Stop tokens?\"]])\n",
    "\n",
    "# Create layer: standardize (lower case, remove punctuation), split, and index\n",
    "vectorizer = TextVectorization(output_mode=\"int\")\n",
    "\n",
    "# Adapt: Learn the vocabulary from the data\n",
    "vectorizer.adapt(training_data)\n",
    "\n",
    "# Inspect vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_vocabulary())\n",
    "\n",
    "# Transform new data\n",
    "print(\"Vectorized:\", vectorizer([[\"This is a sample.\"]]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Interleaving Strategy\n",
    "**Problem:** Reading from one large file one line at a time limits throughput to the speed of a single disk read.\n",
    "**Solution:** `interleave` opens `n_readers` (5 in our code) files at once. It reads a line from File A, then File B, etc. This increases throughput and also adds a layer of shuffling.\n",
    "\n",
    "### 2. Protobuf Serialization\n",
    "**Concept:** The `Example` class is just a container. The real data is in `Features`. \n",
    "* `BytesList` handles strings and binary data (images).\n",
    "* `FloatList` handles float32/float64.\n",
    "* `Int64List` handles integers and booleans.\n",
    "When we call `.SerializeToString()`, the structured object is converted into a compact byte string, which is what gets written to the disk.\n",
    "\n",
    "### 3. Parsing TFRecords\n",
    "Since the TFRecord file is just a stream of bytes, TensorFlow doesn't know what's inside. We must provide a `feature_description` dictionary (a schema) to `tf.io.parse_single_example`. This tells TF: \"Expect a field named 'median_income' which is a float.\"\n",
    "\n",
    "### 4. TextVectorization\n",
    "* **Adaptation:** The layer scans the training text to build a vocabulary (dictionary). It assigns integer ID 1 to the most frequent word, ID 2 to the second, etc.\n",
    "* **Processing:** During inference, it takes a raw string \"This is a sample\", converts it to lowercase, strips punctuation, splits by whitespace, and replaces words with their learned IDs (e.g., `[2, 3, 1, 4]`). This makes the model portable; you don't need a separate tokenizer script in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Data API:** The key to feeding GPUs efficiently. Always use `dataset.prefetch(1)` at the end of your pipeline.\n",
    "* **Parallelism:** Use `num_parallel_calls=tf.data.experimental.AUTOTUNE` in your `.map()` functions to use all CPU cores.\n",
    "* **TFRecords:** Use this format for large datasets. It is sequential, binary, and efficient.\n",
    "* **Preprocessing Layers:** Move preprocessing into the model graph (using `adapt()`) to ensure the model handles raw data identically in training and production.\n",
    "* **One-Hot Encoding:** Use `CategoryEncoding` or `StringLookup` layers instead of pandas `get_dummies`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
