{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Processing Sequences Using RNNs and CNNs\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Predicting the future is one of the most exciting applications of Machine Learning. Whether it is stock prices, weather, or sentences (predicting the next word), the data is sequential. In this chapter, we learn how to handle data where order matters using **Recurrent Neural Networks (RNNs)**, **LSTMs**, **GRUs**, and even **1D CNNs** (WaveNet).\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Recurrent Neurons:** Neurons that feed their output back into themselves (Memory).\n",
    "* **Unrolling through time:** How RNNs are trained using Backpropagation Through Time (BPTT).\n",
    "* **Sequence-to-Sequence vs Sequence-to-Vector:** Different topologies for different tasks.\n",
    "* **The Memory Problem:** Why simple RNNs forget long-term patterns.\n",
    "* **LSTM (Long Short-Term Memory):** The gold standard for handling long sequences. Introduces Forget, Input, and Output gates.\n",
    "* **GRU (Gated Recurrent Unit):** A simplified, faster version of LSTM.\n",
    "* **1D CNNs (WaveNet):** Processing sequences using convolution instead of recurrence.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Generating synthetic time series data.\n",
    "* Building a **SimpleRNN** to predict the next step in a series.\n",
    "* Building a **Deep RNN** (stacked layers).\n",
    "* Implementing **LSTM** and **GRU** layers.\n",
    "* Forecasting 10 steps ahead (Multi-step forecasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. Recurrent Neurons\n",
    "A Feed-Forward network (like MLP or CNN) has no memory; it processes each input independently. An RNN processes sequences by iterating through the sequence elements and maintaining a **state** containing information relative to what it has seen so far.\n",
    "\n",
    "**Math:** $y_{(t)} = \\phi(W_x x_{(t)} + W_y y_{(t-1)} + b)$\n",
    "At time step $t$, the neuron receives the input $x_{(t)}$ AND its own output from the previous time step $y_{(t-1)}$.\n",
    "\n",
    "### 2. The Problem of Long-Term Dependencies\n",
    "Simple RNNs suffer from the vanishing gradient problem severely. If a sequence is long (e.g., 100 steps), the signal from the start of the sequence is lost by the time it reaches the end. It's like trying to remember the first word of a book after reading the whole book.\n",
    "\n",
    "### 3. LSTM (Long Short-Term Memory)\n",
    "Invented in 1997 by Hochreiter and Schmidhuber. It maintains two state vectors:\n",
    "1.  **$h_{(t)}$ (Short-term state):** The output.\n",
    "2.  **$c_{(t)}$ (Long-term state / Cell state):** The conveyor belt that runs straight down the entire chain.\n",
    "\n",
    "It uses **Gates** to regulate flow:\n",
    "* **Forget Gate:** Decides what to throw away from the long-term state.\n",
    "* **Input Gate:** Decides what new information to store.\n",
    "* **Output Gate:** Decides what to output based on the state.\n",
    "\n",
    "### 4. GRU (Gated Recurrent Unit)\n",
    "A simplified LSTM (2014). It merges the cell state and hidden state into one. It has fewer parameters and is faster to train, often matching LSTM performance.\n",
    "\n",
    "### 5. 1D CNNs (WaveNet)\n",
    "Surprisingly, we can use Convolution for sequences. A 1D filter slides over the time axis. By stacking many 1D conv layers with **dilation** (skipping steps), the network can see a very long history (receptive field) without the slowness of sequential processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Generating Synthetic Time Series\n",
    "We generate a sum of two sine waves plus noise to simulate a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # noise\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\"):\n",
    "    plt.plot(series, \".-\")\n",
    "    if y is not None:\n",
    "        plt.plot(n_steps, y, \"bx\", markersize=10)\n",
    "    if y_pred is not None:\n",
    "        plt.plot(n_steps, y_pred, \"ro\")\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(X_valid[0, :, 0], y_valid[0, 0])\n",
    "plt.title(\"Time Series Example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Baseline Metrics\n",
    "Before building complex RNNs, let's establish a baseline.\n",
    "1.  **Naive Forecasting:** Predict the last observed value.\n",
    "2.  **Linear Regression:** A simple Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Naive Forecasting\n",
    "y_pred = X_valid[:, -1]\n",
    "print(\"Naive MSE:\", np.mean(keras.losses.mean_squared_error(y_valid, y_pred)))\n",
    "\n",
    "# 2. Linear Regression (Simple Dense)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"Linear MSE:\", model.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Simple RNN\n",
    "Using the simplest RNN layer. It has a single hidden state looped back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1]) # None allows variable length sequences\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_valid, y_valid))\n",
    "print(\"SimpleRNN MSE:\", model.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Deep RNN\n",
    "Stacking multiple RNN layers. We must set `return_sequences=True` for all layers except the last one, so the next layer receives a 3D sequence, not a 2D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"Deep RNN MSE:\", model.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LSTM and GRU\n",
    "Replacing SimpleRNN with LSTM or GRU to handle longer patterns better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"LSTM MSE:\", model.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Predicting 10 Steps Ahead (Sequence-to-Vector)\n",
    "Instead of predicting just the next value ($t+1$), we predict the next 10 values ($t+1$ to $t+10$). We need to regenerate the target data $Y$ to be a vector of 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new data where Y contains 10 future steps\n",
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20),\n",
    "    keras.layers.Dense(10) # Output layer has 10 neurons now\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X_train, Y_train, epochs=20, verbose=0)\n",
    "print(\"Multi-step GRU MSE:\", model.evaluate(X_valid, Y_valid, verbose=0))\n",
    "\n",
    "# Visualization of 10-step forecast\n",
    "Y_pred = model.predict(X_new)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_steps), X_test[0, :, 0], \"b.-\", label=\"History\")\n",
    "plt.plot(np.arange(n_steps, n_steps + 10), Y_test[0], \"gx\", label=\"Actual Future\")\n",
    "plt.plot(np.arange(n_steps, n_steps + 10), Y_pred[0], \"ro\", label=\"Forecast\")\n",
    "plt.legend()\n",
    "plt.title(\"10-Step Forecast\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Shape of Data\n",
    "RNNs in Keras expect 3D input: `[batch_size, time_steps, dimensionality]`.\n",
    "* `batch_size`: Number of samples (e.g., 32).\n",
    "* `time_steps`: Length of the sequence (e.g., 50).\n",
    "* `dimensionality`: Number of features per step. For univariate time series (just value), it is 1. For multivariate (e.g., Price + Temperature), it could be 2+.\n",
    "\n",
    "### 2. Return Sequences\n",
    "This is the most common confusion point.\n",
    "* `return_sequences=False` (Default): The layer outputs a 2D array `[batch_size, units]`. It only returns the output of the *last* time step. Used for the final layer or before a Dense layer.\n",
    "* `return_sequences=True`: The layer outputs a 3D array `[batch_size, time_steps, units]`. It outputs the hidden state for *every* time step. This is required when stacking RNN layers, so the next RNN layer has a sequence to process.\n",
    "\n",
    "### 3. LSTM Internals\n",
    "\n",
    "The LSTM cell has a \"highway\" for the cell state $c_{(t)}$ to pass through with minimal interference (multiplication by 1 or 0). This allows gradients to flow back many steps without vanishing, solving the memory problem.\n",
    "\n",
    "### 4. Sequence-to-Vector vs Sequence-to-Sequence\n",
    "* **Seq-to-Vec:** Input is a sequence, output is a vector (e.g., Sentiment Analysis, predicting the next value). We ignore intermediate outputs.\n",
    "* **Seq-to-Seq:** Input is a sequence, output is a sequence (e.g., Translation, Frame-by-frame video classification). We use `return_sequences=True` and `TimeDistributed(Dense(...))` to apply a Dense layer to every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **RNNs** are designed for sequential data.\n",
    "* **SimpleRNN** is generally too weak for real tasks due to vanishing gradients.\n",
    "* **LSTM** and **GRU** are the standard solutions. They use gates to learn what to remember and what to forget.\n",
    "* **Stacking:** Deep RNNs work better than shallow ones, but training is slow.\n",
    "* **Forecasting:** You can predict 1 step ahead or N steps ahead.\n",
    "* **1D CNNs (WaveNet):** A powerful alternative to RNNs. They can handle very long sequences efficiently using dilated convolutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
