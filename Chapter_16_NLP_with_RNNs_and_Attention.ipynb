{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** Natural Language Processing (NLP) is one of the most active fields in AI. In this chapter, we will build systems that can generate text (like Shakespeare), translate languages (English to Spanish), and understand context using **Attention Mechanisms**. We will culminate with the **Transformer** architecture, the foundation of modern Large Language Models.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Char-RNN:** Generating text character by character.\n",
    "* **Stateful RNNs:** Preserving hidden state across batches for long sequences.\n",
    "* **Sentiment Analysis:** Classifying text (positive/negative).\n",
    "* **Encoder-Decoder Network:** The standard architecture for Neural Machine Translation (NMT).\n",
    "* **Bidirectional RNNs:** Reading text forward and backward.\n",
    "* **Beam Search:** Finding the most likely sequence of words, not just the greedy choice.\n",
    "* **Attention Mechanisms:** Allowing the decoder to \"focus\" on specific parts of the input sentence.\n",
    "* **The Transformer:** An architecture based entirely on Attention (Self-Attention, Multi-Head Attention), abandoning RNNs.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Building a character-level text generation model.\n",
    "* Implementing a basic Encoder-Decoder for translation.\n",
    "* Using **TensorFlow Addons (tfa)** for Seq2Seq models.\n",
    "* Implementing **Positional Encoding** and **Multi-Head Attention** from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. Char-RNN & Stateful RNNs\n",
    "To generate text, we can train an RNN to predict the next character given a sequence of previous characters. \n",
    "* **Stateless RNN:** The hidden state is reset to zero at the start of every batch. Good for independent sentences.\n",
    "* **Stateful RNN:** The final hidden state of batch $i$ is used as the initial state of batch $i+1$. This allows the network to learn patterns that span across batches (e.g., long chapters).\n",
    "\n",
    "### 2. Encoder-Decoder Architecture\n",
    "Used for translation. \n",
    "* **Encoder:** An RNN that reads the input sentence (e.g., \"Hello\") and condenses it into a single vector (the final hidden state).\n",
    "* **Decoder:** An RNN that takes that vector and generates the translation (\"Hola\") step-by-step.\n",
    "* **Problem:** Condensing a long sentence into a single vector causes information loss (the \"bottleneck\" problem).\n",
    "\n",
    "### 3. Attention Mechanisms\n",
    "Bahdanau et al. (2014) introduced Attention to solve the bottleneck. Instead of just sending the final state to the decoder, we send *all* encoder outputs. At each step, the decoder calculates a weighted sum of these outputs, focusing (attending) on the words most relevant to the current word it is generating.\n",
    "\n",
    "### 4. The Transformer (Vaswani et al., 2017)\n",
    "The paper \"Attention Is All You Need\" proposed removing RNNs entirely. \n",
    "* **Positional Encoding:** Since there is no recurrence, we must inject information about word order (index 1, 2, 3...) mathematically.\n",
    "* **Self-Attention:** Each word in the sentence looks at every other word to understand context (e.g., \"Bank\" looks at \"River\" or \"Money\").\n",
    "* **Multi-Head Attention:** Running several self-attention layers in parallel to capture different types of relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Char-RNN: Generating Shakespearean Text\n",
    "We download the Shakespeare dataset, tokenize it by character, and train a GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = keras.utils.get_file(\"shakespeare.txt\", \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Tokenizer: Char to Int\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])\n",
    "\n",
    "# Encode full text\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(f\"Total characters: {dataset_size}\")\n",
    "print(f\"Unique characters: {max_id}\")\n",
    "\n",
    "# Create Dataset (Windowing)\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id),\n",
    "                              Y_batch))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Build Model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "# history = model.fit(dataset, epochs=5) # Skipped for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encoder-Decoder for Neural Machine Translation (NMT)\n",
    "We will build a simple English-to-Spanish translator. We use `Bidirectional` LSTM for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embed_size = 10\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "# Encoder\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder (Initialized with Encoder states)\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_states)\n",
    "\n",
    "# Output\n",
    "output_layer = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Positional Encoding (Transformer Component)\n",
    "Since Transformers have no recurrence, we add sine/cosine waves to the embeddings to represent position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, 0::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[1], :shape[2]]\n",
    "\n",
    "# Visualizing Positional Encodings\n",
    "max_steps = 201\n",
    "max_dims = 512\n",
    "pos_emb = PositionalEncoding(max_steps, max_dims)\n",
    "PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.pcolormesh(PE[0], cmap='RdBu')\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Token Position')\n",
    "plt.colorbar()\n",
    "plt.title(\"Positional Encoding Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Multi-Head Attention (Transformer Component)\n",
    "The core logic of the Transformer: `Attention(Q, K, V) = softmax(QK^T / sqrt(d)) * V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self, n_heads, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.wq = keras.layers.Dense(d_model)\n",
    "        self.wk = keras.layers.Dense(d_model)\n",
    "        self.wv = keras.layers.Dense(d_model)\n",
    "        self.dense = keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.n_heads, self.d_head))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        qs = self.split_heads(self.wq(q), batch_size)\n",
    "        ks = self.split_heads(self.wk(k), batch_size)\n",
    "        vs = self.split_heads(self.wv(v), batch_size)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        matmul_qk = tf.matmul(qs, ks, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(ks)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9) \n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, vs)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        original_size_output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        return self.dense(original_size_output)\n",
    "\n",
    "print(\"MultiHeadAttention layer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Data Pipeline for Char-RNN\n",
    "**Input:** A long string of text.\n",
    "**Process:**\n",
    "1.  **Windowing:** We chop the text into overlapping windows of 101 characters.\n",
    "2.  **Split:** The first 100 characters are the input ($X$), the characters from index 1 to 101 are the target ($Y$). Essentially, at each step $t$, the model predicts char $t+1$.\n",
    "3.  **One-Hot:** We convert integer IDs to sparse vectors.\n",
    "\n",
    "### 2. Encoder-Decoder Flow\n",
    "1.  **Encoder:** The LSTM processes the input sequence (English). We discard the `encoder_outputs` and keep only the final `state_h` and `state_c`. These states represent the \"meaning\" or \"context\" of the sentence.\n",
    "2.  **Bridge:** We pass these states to the Decoder as its `initial_state`. \n",
    "3.  **Decoder:** The LSTM starts with the context vector and the `<start>` token. It generates the first translated word (Spanish). This word is then fed back as input for the next step (during inference).\n",
    "\n",
    "### 3. Why Positional Encoding?\n",
    "RNNs process \"I\" then \"am\" then \"happy\". They know \"I\" comes first because it was processed at step 0. \n",
    "Transformers process \"I\", \"am\", and \"happy\" simultaneously (in parallel). Without Positional Encoding, the model would see \"I am happy\" and \"Happy am I\" as identical \"bags of words\". The sine/cosine waves add a unique signature to each position index so the model can distinguish order.\n",
    "\n",
    "### 4. Attention Mechanism Logic\n",
    "**Query (Q), Key (K), Value (V):**\n",
    "* Analogy: Searching a library database.\n",
    "* **Query:** What you are looking for (e.g., \"Books about Space\").\n",
    "* **Key:** The labels on the books in the library (e.g., \"Science\", \"Cooking\", \"Space\").\n",
    "* **Value:** The content of the books.\n",
    "* **Dot Product (QK):** Checks similarity between Query and Keys. \"Space\" matches \"Space\" (High score).\n",
    "* **Softmax:** Converts scores to probabilities.\n",
    "* **Output:** Weighted sum of Values. You get the content of the Space books.\n",
    "In Self-Attention, Q, K, and V all come from the same word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Char-RNN** works surprisingly well for generating text but struggles with long-term coherence.\n",
    "* **Encoder-Decoder** is the standard for Seq2Seq tasks like translation.\n",
    "* **Beam Search** improves translation quality by exploring multiple potential translations simultaneously.\n",
    "* **Attention** solves the bottleneck problem by letting the decoder look at the entire source sentence dynamically.\n",
    "* **The Transformer** is the current state-of-the-art. It uses Multi-Head Attention and Positional Encodings to process sequences in parallel, enabling massive scale (BERT, GPT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
