{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
    "\n",
    "## 1. Chapter Overview\n",
    "**Goal:** This chapter marks the transition from traditional Machine Learning to **Deep Learning**. We will explore Artificial Neural Networks (ANNs), starting from the simplest architecture (Perceptron) to Multi-Layer Perceptrons (MLPs). We will use **Keras**, a high-level API running on top of TensorFlow, to build models capable of classifying fashion images and predicting housing prices.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Perceptrons & TLUs:** The basic logic units of neural networks.\n",
    "* **Multi-Layer Perceptron (MLP):** Stacking layers to solve non-linear problems (XOR problem).\n",
    "* **Backpropagation:** The revolutionary training algorithm that computes gradients efficiently.\n",
    "* **Activation Functions:** Why we need ReLU, Sigmoid, or Softmax.\n",
    "* **Keras API:**\n",
    "    * *Sequential API:* For simple stacks of layers.\n",
    "    * *Functional API:* For complex topologies (Wide & Deep).\n",
    "    * *Subclassing API:* For full control (research level).\n",
    "* **Hyperparameter Tuning:** Using RandomizedSearch to find optimal neuron counts and learning rates.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Building an Image Classifier for the **Fashion MNIST** dataset.\n",
    "* Building a Regressor for California housing data.\n",
    "* Using Callbacks (EarlyStopping, ModelCheckpoint) to prevent overfitting.\n",
    "* Saving and loading Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanation (In-Depth)\n",
    "\n",
    "### 1. From Biological to Artificial (The Perceptron)\n",
    "The basic idea of ANNs is inspired by biological neurons. Neurons receive input signals via dendrites, process them, and if the signal is strong enough, fire an output signal via the axon.\n",
    "\n",
    "**Perceptron (Frank Rosenblatt, 1957):**\n",
    "This is the simplest ANN architecture. It consists of a single layer of **Threshold Logic Units (TLUs)**. The inputs are numbers, each having a weight ($w$). The TLU computes a weighted sum ($z = w_1 x_1 + \\dots + w_n x_n + bias$), then applies a *step function*.\n",
    "* Limitation: Perceptrons can only solve linear problems. They fail at the simple XOR problem.\n",
    "\n",
    "### 2. Multi-Layer Perceptron (MLP) and Backpropagation\n",
    "To overcome the limitations of the Perceptron, we stack multiple layers of TLUs. This structure is called an MLP:\n",
    "1.  **Input Layer:** Receives the data features.\n",
    "2.  **Hidden Layers:** One or more layers in the middle that transform the representation.\n",
    "3.  **Output Layer:** Produces the final prediction.\n",
    "\n",
    "**Backpropagation (Rumelhart, Hinton, Williams, 1986):**\n",
    "How do we train such a deep network? Backpropagation is key. It works in two main passes for each training batch:\n",
    "1.  **Forward Pass:** Data flows from input to output, generating predictions, and the error is calculated using a *Loss Function*.\n",
    "2.  **Backward Pass:** The algorithm computes the gradient of the error with regard to every parameter (weight) by moving backward from output to input (using the *Chain Rule* of calculus). It tells us how much each weight contributed to the error.\n",
    "3.  **Update:** A Gradient Descent step uses these gradients to update the weights to reduce the error.\n",
    "\n",
    "### 3. Activation Functions\n",
    "For an MLP to learn non-linear patterns, we **must** use non-linear activation functions between linear layers.\n",
    "* **Sigmoid:** Squashes output to 0-1. Good for probabilities, but suffers from *vanishing gradients* in deep layers.\n",
    "* **ReLU (Rectified Linear Unit):** $ReLU(z) = max(0, z)$. Fast to compute and does not saturate for positive values. It is the de-facto standard for hidden layers.\n",
    "* **Softmax:** Used in the output layer for multiclass classification. Ensures probabilities sum to 1.\n",
    "\n",
    "### 4. Keras API\n",
    "TensorFlow 2 adopted Keras as its official high-level API. It is user-friendly:\n",
    "* **Sequential API:** Easiest. Just `model.add(Layer)`. Good for 90% of cases.\n",
    "* **Functional API:** More flexible. Allows multiple inputs/outputs and non-linear topologies (like Wide & Deep).\n",
    "* **Subclassing API:** Most complex but flexible. You write your own Python class. Used for research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Reproduction\n",
    "\n",
    "### 3.1 Building an Image Classifier (Fashion MNIST)\n",
    "We use the Fashion MNIST dataset (70,000 grayscale images of 10 fashion categories) because it is slightly harder than the classic MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# 2. Validation Split & Normalization (Scaling)\n",
    "# Pixel values are 0-255. We scale them to 0-1 for Neural Networks.\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Example class:\", class_names[y_train[0]])\n",
    "\n",
    "# Visualize one image\n",
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating a Model using Sequential API\n",
    "We will build an MLP with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), # Flattens 2D (28x28) to 1D (784)\n",
    "    keras.layers.Dense(300, activation=\"relu\"), # Hidden Layer 1: 300 neurons, ReLU\n",
    "    keras.layers.Dense(100, activation=\"relu\"), # Hidden Layer 2: 100 neurons, ReLU\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # Output Layer: 10 classes, Softmax\n",
    "])\n",
    "\n",
    "# View architecture summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compile and Train\n",
    "* **Loss:** `sparse_categorical_crossentropy` because our labels are integers (0-9), not one-hot vectors.\n",
    "* **Optimizer:** `sgd` (Stochastic Gradient Descent).\n",
    "* **Metrics:** `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model (this might take a few seconds/minutes)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualizing Learning Curves\n",
    "The `history` object stores loss and accuracy data during training. We plot it to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on Test Set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Prediction\n",
    "Using the model to predict class probabilities on new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "print(\"Probabilities:\\n\", y_proba.round(2))\n",
    "\n",
    "# Get class with highest probability\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "print(\"Predictions:\", np.array(class_names)[y_pred])\n",
    "print(\"Actual Labels:\", np.array(class_names)[y_test[:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Regression MLP (Functional API)\n",
    "For regression (predicting California housing prices), the MLP structure differs:\n",
    "* Output layer has only **1 neuron**.\n",
    "* **No activation function** at output (we want continuous values).\n",
    "* Loss function: **MSE**.\n",
    "\n",
    "We will also use the **Functional API** to build a **Wide & Deep** architecture. It connects part of the input directly to the output (Wide) and part through deep layers (Deep). This allows learning both simple (linear) patterns and complex (deep) patterns simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load & Split Data\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Scaling (CRITICAL for Neural Networks!)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# --- Functional API ---\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "# Concatenate input directly with output of hidden layer 2\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model_func = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "model_func.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "\n",
    "print(\"Training Wide & Deep Model...\")\n",
    "history = model_func.fit(X_train, y_train, epochs=20,\n",
    "                         validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Callbacks\n",
    "What if we train for too long? Overfitting. Instead of guessing epochs, use `EarlyStopping`. We also use `ModelCheckpoint` to save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "\n",
    "# Patience=10: Stop if val_loss doesn't improve for 10 consecutive epochs\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model_func.fit(X_train, y_train, epochs=100,\n",
    "               validation_data=(X_valid, y_valid),\n",
    "               callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step Explanation\n",
    "\n",
    "### 1. Image Data Preprocessing\n",
    "**Input:** Images of `28x28` pixels with values `0-255`.\n",
    "**Process:** Neural Networks converge faster with small input values (around 0-1). Thus, we divide by `255.0`. Forgetting this leads to exploding gradients or stalled training.\n",
    "\n",
    "### 2. Classification Architecture\n",
    "* `Flatten`: Has no parameters. It just reshapes the 2D matrix into a long 1D vector. It bridges the gap between image data and Dense layers.\n",
    "* `Dense`: Fully Connected layers. Every neuron connects to every neuron in the previous layer.\n",
    "* `ReLU`: Without this, a stack of Dense layers is mathematically equivalent to a single Linear layer. ReLU adds non-linearity, allowing the model to learn complex shapes.\n",
    "* `Softmax`: Converts raw scores (logits) into a probability distribution (sum = 1).\n",
    "\n",
    "### 3. Wide & Deep Architecture\n",
    "In the Functional API code, `Concatenate` merges the raw inputs with the processed deep features. \n",
    "* **Deep path** learns abstract patterns.\n",
    "* **Wide path** (direct connection) learns simple rules.\n",
    "* This often outperforms standard MLP on tabular data.\n",
    "\n",
    "### 4. Early Stopping\n",
    "This is automatic regularization. Instead of manual monitoring, if `val_loss` stops improving for 10 epochs, the callback stops training and restores the best weights, preventing wasted resources and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chapter Summary\n",
    "\n",
    "* **Deep Learning** uses multi-layer neural networks to learn complex data representations.\n",
    "* **Keras** is the user-friendly, high-level API for TensorFlow.\n",
    "* **Sequential API** is great for simple stacks.\n",
    "* **Functional API** is needed for complex topologies (branching, multiple inputs).\n",
    "* **Preprocessing:** Always scale data (StandardScaler) before feeding into a Neural Net.\n",
    "* **Loss Function:** Use `sparse_categorical_crossentropy` for integer classification, `mse` for regression.\n",
    "* **Activation:** `ReLU` for hidden layers; `Softmax` (classification) or `Linear` (regression) for output.\n",
    "* **Optimization:** Avoid overfitting using `EarlyStopping` and save models with `ModelCheckpoint`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
